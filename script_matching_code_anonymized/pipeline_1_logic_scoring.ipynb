{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipeline Stage 1: Initial Filtering and Logic Scoring</h2>\n",
    "\n",
    "<p>Welcome to the first stage in the matching pipeline! At the end of the pipeline, each student will have a 1 to 2 strong matches. Each company will have a minimum of 2 strong matches and a maximum of 4 strong matches. This fully automates the manual matching process that was done in the past. Let's get started!</p>\n",
    "\n",
    "<p>The table I will be using throughout these pipelines is a cross join of the position profiles and student applications. This means that every student is matched with every company for each year of matching. The goal of each stage of the pipeline is to reduce these matches so each student is matched with their most aligned company, and each company gets a pool of quality candidates.</p>\n",
    "\n",
    "<p><strong>Note:</strong> The data used in this pipeline was generated by AI to ensure confidentiality of the original data. As a result, the outcomes may differ from the actual dataset. However, the findings presented in the Technical White Paper are based on real student and company data, thoroughly anonymized to remove any identifying details.</p>\n",
    "\n",
    "<h3>Description of the table:</h3>\n",
    "<ul>\n",
    "  <li><strong>Student application columns:</strong> Marked with a <code>'stu_'</code> prefix.</li>\n",
    "  <li><strong>Position profile columns:</strong> Marked with a <code>'pos_'</code> prefix.</li>\n",
    "  <li><strong>Year columns:</strong> I added a <code>'stu_Year'</code> and <code>'pos_Year'</code> column to the table to filter out students that are not in the same year as the position.</li>\n",
    "  <li><strong>Unstructured text data:</strong> The main columns used in the pipeline are any unstructured text data. This includes the <code>'stu_Resume Text'</code> and <code>'pos_Job Description text'</code>, and any text field data added when students and companies created their profiles.</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Note:</strong> This project uses a custom library I created called <code>'utils.py'</code>, found in the <code>'my_package'</code> folder of this project. This library contains functions that are used throughout the pipeline. Make sure to download the library into your Python environment before running this script. This project used python 3.11.9</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~\n",
    "import pandas as pd\n",
    "from mypackage.utils import *\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the path to the project here. If you run the code, any tables will be loaded automatically.\n",
    "# The load path will carry over across all notebooks in the project.\n",
    "path_to_project = '/path/to/project/script_matching_code_anonymized'\n",
    "\n",
    "# Function to save the path to a JSON file\n",
    "save_project_path(path_to_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SP table shown below is a cross-join between the student application table and the position profile table. It has been scored using the original weighted scoring method for matching. The matching scores are located in the \"Total Points\" and \"Total Point Breakup\" columns and will be used to compare the results of the original matching pipeline with the AI-based matching pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{path_to_project}/data/SP_table/SP1.parquet'\n",
    "SP = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code that mimics the same filtering system we use currently. It filters out students with less then 2 skill matches. \n",
    "If you run the code below and look at the results, it highlights one of the main problems with the current system; over and under matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count stu_Z1 + stu_Z2 occurrences\n",
    "def count_stu_Z1_Z2_skills(total_points_breakup):\n",
    "    return total_points_breakup.count('stu_Z1') + total_points_breakup.count('stu_Z2')\n",
    "\n",
    "# Apply the function to count stu_Z2 skills in each row\n",
    "SP['stu_skill_count'] = SP['Total Points BreakUp'].apply(count_stu_Z1_Z2_skills)\n",
    "\n",
    "# Filter rows where stu_Z2_count is less than 3\n",
    "SP_filtered = SP[SP['stu_skill_count'] >= 3]\n",
    "\n",
    "# Calculate the number of matches for each company\n",
    "pos_match_counts = SP_filtered['pos_(Do Not Modify) Job Posting'].value_counts().reset_index()\n",
    "pos_match_counts.columns = ['pos_(Do Not Modify) Job Posting', 'pos_Number of Matches']\n",
    "\n",
    "# Merge the match counts back into the combined DataFrame\n",
    "SP_filtered = SP_filtered.merge(pos_match_counts, on='pos_(Do Not Modify) Job Posting', how='left')\n",
    "\n",
    "# Calculate the number of matches for each student\n",
    "stu_match_counts = SP_filtered['stu_(Do Not Modify) Application'].value_counts().reset_index()\n",
    "stu_match_counts.columns = ['stu_(Do Not Modify) Application', 'stu_Number of Matches']\n",
    "\n",
    "# Merge the student match counts back into the combined DataFrame\n",
    "SP_filtered = SP_filtered.merge(stu_match_counts, on='stu_(Do Not Modify) Application', how='left')\n",
    "\n",
    "# Create the 'pos_Difficult to Match' column\n",
    "SP_filtered['pos_Difficult to Match'] = SP_filtered['pos_Number of Matches'].apply(lambda x: 'Yes' if x < 10 else 'No')\n",
    "\n",
    "# Drop the temporary stu_Z2_count column\n",
    "SP_filtered = SP_filtered.drop(columns=['stu_skill_count'])\n",
    "\n",
    "\n",
    "# Identify job postings that have all their rows filtered out\n",
    "hard_to_match_companies = SP[~SP['pos_(Do Not Modify) Job Posting'].isin(SP_filtered['pos_(Do Not Modify) Job Posting'].unique())]\n",
    "\n",
    "# Process 'hmat' DataFrame to retain only pos_ columns and drop duplicates\n",
    "hmat = hard_to_match_companies.loc[:, hard_to_match_companies.columns.str.startswith('pos_')].drop_duplicates()\n",
    "\n",
    "print(hmat.shape)\n",
    "\n",
    "# Add columns to 'hmat' DataFrame\n",
    "hmat['stu_(Do Not Modify) Application'] = \"\"\n",
    "hmat['Total Points'] = \"\"\n",
    "hmat['Total Points BreakUp'] = \"\"\n",
    "\n",
    "# Combine 'hmat' with the filtered DataFrame\n",
    "SP_combined = pd.concat([SP_filtered, hmat], ignore_index=True)\n",
    "\n",
    "# set blank values in 'pos_Number of Matches' to 0\n",
    "SP_combined['pos_Number of Matches'] = SP_combined['pos_Number of Matches'].fillna(0)\n",
    "\n",
    "# if the 'stu_(Do Not Modify) Application' is blank, set the 'pos_Number of Matches' to 0\n",
    "SP_combined.loc[SP_combined['stu_(Do Not Modify) Application'] == '', 'pos_Number of Matches'] = 0\n",
    "\n",
    "# convert the 'pos_Number of Matches' column to integer type\n",
    "SP_combined['pos_Number of Matches'] = SP_combined['pos_Number of Matches'].astype(int)\n",
    "\n",
    "# if the 'stu_(Do Not Modify) Application' is blank, set the 'Total Points' to 0\n",
    "SP_combined.loc[SP_combined['stu_(Do Not Modify) Application'] == '', 'Total Points'] = 0\n",
    "\n",
    "# convert the 'Total Points' column to integer type\n",
    "SP_combined['Total Points'] = SP_combined['Total Points'].astype(int)\n",
    "\n",
    "# The resulting DataFrame SP_combined now contains the desired columns\n",
    "SP_filtered = SP_combined\n",
    "\n",
    "# save the SP_filtered table to a parquet file\n",
    "SP_filtered_path = f'{path_to_project}/data/SP_table/SP_filtered.parquet'\n",
    "SP_filtered.to_parquet(SP_filtered_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting table from this code will show you the number of matches for each position profile after filtering out ineligible students and students with less than 3 skills.\n",
    "\n",
    "cols = ''' \n",
    "pos_Company\n",
    "pos_Name\n",
    "pos_(Do Not Modify) Job Posting\n",
    "pos_Number of Matches\n",
    "'''\n",
    "cols = as_list(cols)\n",
    "\n",
    "pos_num_matches = SP_filtered[cols]\n",
    "\n",
    "# drop duplicates\n",
    "pos_num_matches = pos_num_matches.drop_duplicates()\n",
    "\n",
    "# sort by 'pos_Number of Matches' in descending order\n",
    "pos_num_matches = pos_num_matches.sort_values(by='pos_Number of Matches', ascending=False)\n",
    "\n",
    "# Display the table\n",
    "cols_to_display = ['pos_Company', 'pos_Name', 'pos_Number of Matches']\n",
    "pretty_print(pos_num_matches[cols_to_display])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, several companies have 0 to 2 matches, requiring a manual process of matching students to these companies. This takes a lot of time and effort. The goal of the pipeline is to eliminate this manual process and automate the matching process, providing each company with a pool of quality candidates. Some companies are also clearly overmatched, with more then 200 matches. This makes it difficult to find the best candidates for the positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting table from this code will show you the number of matches for each student application after filtering out ineligible students and students with less than 3 skills.\n",
    "\n",
    "cols = '''  \n",
    "stu_(Do Not Modify) Application\n",
    "stu_Legal Name\n",
    "stu_Number of Matches\n",
    "'''\n",
    "\n",
    "cols = as_list(cols)\n",
    "\n",
    "stu_num_matches = SP_filtered[cols]\n",
    "\n",
    "# drop duplicates\n",
    "stu_num_matches = stu_num_matches.drop_duplicates()\n",
    "\n",
    "# sort by 'stu_Number of Matches' in descending order\n",
    "stu_num_matches = stu_num_matches.sort_values(by='stu_Number of Matches', ascending=False)\n",
    "\n",
    "# drop nan values\n",
    "stu_num_matches = stu_num_matches.dropna()\n",
    "\n",
    "cols_to_display = ['stu_Legal Name', 'stu_Number of Matches']\n",
    "pretty_print(stu_num_matches[cols_to_display])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table also shows an uneven distribution of the number of matches for student applications. Some students have over 50 matches, while others have only 1 match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting histograms will show the distribution of the number of matches for position profiles and student applications.\n",
    "\n",
    "# Extract the 'pos_Number of Matches' column\n",
    "data = pos_num_matches['pos_Number of Matches']\n",
    "\n",
    "# Plot the histogram of the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "count, bins, ignored = plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# Fit a normal distribution to the data\n",
    "mu, std = norm.fit(data)\n",
    "\n",
    "# Plot the PDF of the fitted normal distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = 'Distribution of Number of Matches for Position Profiles'\n",
    "plt.title(title)\n",
    "\n",
    "plt.xlabel('Number of Matches')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Extract the 'stu_Number of Matches' column\n",
    "data = stu_num_matches['stu_Number of Matches']\n",
    "\n",
    "# Plot the histogram of the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "count, bins, ignored = plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# Fit a normal distribution to the data\n",
    "mu, std = norm.fit(data)\n",
    "\n",
    "# Plot the PDF of the fitted normal distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = 'Distribution of Number of Matches for Student Applications'\n",
    "plt.title(title)\n",
    "\n",
    "plt.xlabel('Number of Matches')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show the distribution of the number of matches for position profiles and student applications. A few position profiles have a large number of matches, while most have fewer matches. The distribution for student applications is similar, with a few applications having a large number of matches and most having fewer matches. This causes the 'hard to match' comapnies issue where someone has to manually find matches for companies where the current system brings no matches. This can also result in less then optimal matches when one company has a large number of matches. It's difficult to sort through all the matches to find the best ones. The goal with the new pipelines is to flatten the distribution of matches so that there are fewer outliers and more evenly distributed matches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
