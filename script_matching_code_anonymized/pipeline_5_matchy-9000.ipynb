{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Stage 5: Matchy-9000\n",
    "\n",
    "Welcome to Pipeline 5! In this stage, we will be using the data from Pipeline 3 to generate our final matches. The process involves leveraging the OpenAI API to generate a similarity score between job profiles and student profiles. By providing the API with both a job profile and a student profile, it returns a similarity score that helps us identify the best matches. This stage refines the matching process, ensuring that the most compatible candidates are paired with the most suitable job opportunities.\n",
    "\n",
    "<p>Below the Matchy-9000 code, you will find previously generated results that you can review without needing to run Matchy-9000.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait\n",
    "import concurrent.futures\n",
    "import os \n",
    "import time \n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "from IPython import get_ipython\n",
    "from IPython.display import clear_output, display\n",
    "import ast\n",
    "from mypackage.utils import *\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI, RateLimitError, AzureOpenAI\n",
    "import threading\n",
    "import tiktoken\n",
    "import ipywidgets as widgets\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Error handling\n",
    "class RateLimitError(Exception):\n",
    "    pass\n",
    "class TimeoutException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>API Key Setup</h3>\n",
    "\n",
    "<p>Below, you need to add your own API key. You can access these via Azure or OpenAI. Please note that they do cost money to use.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # OpenAI API key setup\n",
    "# client = AzureOpenAI(\n",
    "#     azure_endpoint = 'endpoint_here',\n",
    "#     api_key= 'api_key_here',\n",
    "#     api_version=\"2024-05-01-preview\"\n",
    "# )\n",
    "\n",
    "\n",
    "# My API key\n",
    "client = OpenAI(api_key='Your API key here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the project path\n",
    "path_to_project = load_project_path()\n",
    "\n",
    "if path_to_project:\n",
    "    print(f\"Project path loaded: {path_to_project}\")\n",
    "else:\n",
    "    print(\"Please set the project path in the initial notebook.\")\n",
    "    \n",
    "SP_path = f'{path_to_project}/data/SP_table/SP4_post_keyword_filtering.parquet' \n",
    "SP = pd.read_parquet(SP_path)\n",
    "print(SP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Instructions\n",
    "\n",
    "These are the instructions currently being provided to the API. They were drafted with the intent to guide the API in evaluating the alignment between internship applicants and specific job descriptions. However, these instructions should undergo an iterative process of improvement to enhance their effectiveness.\n",
    "\n",
    "#### Key Features and Considerations\n",
    "\n",
    "1. **Referencing**: \n",
    "   - One of the critical elements of these instructions is the emphasis on referencing. By including explicit instructions on how to reference the information, we can minimize hallucinations and ensure that the AI generates responses that are grounded in the provided data. \n",
    "   - The use of references, such as line numbers from both student profiles and job descriptions, helps maintain traceability and transparency in the AI's responses.\n",
    "\n",
    "2. **Numbering Lines**:\n",
    "   - I have found that numbering each line of the information provided to the AI is an effective method for enabling accurate referencing. This approach ensures that the AI can precisely identify and relate specific details from the input data to the task at hand.\n",
    "   - Currently, the text being sent to the AI does not include numbered lines. It would be beneficial to refine the function that prepares the text to incorporate line numbering. This refinement would further enhance the accuracy of the AI's referencing and reduce the risk of errors.\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "- **Iterative Improvement**: The instructions should be regularly reviewed and updated based on feedback and results. This will help identify areas where the AI's performance can be enhanced and ensure that the instructions remain aligned with the project's goals.\n",
    "- **Implementing Line Numbering**: Refining the function that prepares the text to include line numbers is a recommended next step. This will improve the AI's ability to reference information accurately and reduce the potential for generating content that is not directly tied to the provided data.\n",
    "\n",
    "By following these recommendations, the instructions can be made more robust, leading to more accurate and reliable outcomes from the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    " Task:\n",
    "\n",
    "Your task is to see how well internship applicants align with a specific job. I will be sending you different student profiles (which include essays, resumes, and other information about the student), and job descriptions. You will compute an alignment score between the student profile and the job description. The slignment will be broken down into four categories:\n",
    "\n",
    "1. Company and Industry Alignment\n",
    "2. Job Role and Responsibilities vs. Applicant Experience\n",
    "3. Education, Technical Skills, and Tools\n",
    "4. Values, perks, development opportunities, and Company Culture Alignment:\n",
    "\n",
    "\n",
    "Alignment Score Notes:\n",
    "- You will also calculate the overall similarity score by averaging the alignment scores from each category. \n",
    "\n",
    "Referencing Notes:\n",
    "- When analyzing the alignment between the job description and the student profile, reference the line numbers from which the information is taken.\n",
    "- Use the following format for referencing:\n",
    "  - For information from the student profile, label it as (SP line #).\n",
    "  - For information from the job description, label it as (JD line #).\n",
    "\n",
    "Examples of referencing:\n",
    "1. If a student profile on line 5 states, \"5. I am good with SQL,\" and the job description on line 17 mentions, \"17. We need someone who knows SQL\":\n",
    "   - *Output:* \"The student's knowledge of SQL (SP line 5) aligns well with the job's requirement for someone skilled in this area (JD line 17).\"\n",
    "2. If a student profile expresses a strong preference for working in the beauty industry, saying on line 3, \"3. I really only want to work in the beauty industry.\", and the job description indicates the job is in the accounting industry on line 19 by stating, \"19. This job is in the accounting industry.\":\n",
    "   - *Output:* \"The student's desire to work in the beauty industry (SP line 3) does not align with the job's focus on the accounting industry (JD line 19).\"\n",
    "\n",
    "Additional Notes:\n",
    "- It is EXTREAMLY important to adhead to the format provided under 'Expected Output:'.\n",
    "- Do not include any additional explanations or words in the output beyond what is outlines under 'Expected Output:'\n",
    "- All outputs should be in plain text without using any special formatting, such as HTML, Markdown, LaTeX, or any other formatting tags. Your output should be in plain text, following the formatting outlined under 'Expected Output:'. \n",
    "- Do not use any bold text (i.e. bold) or formatted headers. \n",
    "\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "Student Name: [Student Name Goes Here]\n",
    "\n",
    "Category Breakdown\n",
    "\n",
    "Company and Industry Alignment:\n",
    "- Job Description: [Briefly describe the industry focus and mission of the company.]\n",
    "- Applicant: [Briefly describe the applicant's industry interests and aspirations.\n",
    "- Summary of Alignment: Provide a concise summary of how well the applicant's interests align with the company's industry and mission.\n",
    "- Alignment Score: X.0/10\n",
    "\n",
    "2. Job Role and Responsibilities vs. Applicant Experience:\n",
    "- Job Description: [Outline the key responsibilities and roles in the job description.]\n",
    "- Applicant: [Summarize the applicant's relevant experience and past roles.]\n",
    "- Summary of Alignment: [Provide a summary of how well the applicant's experience matches the job responsibilities.]\n",
    "- Alignment Score: X.0/10\n",
    "[\n",
    "3. Education, Skills, and Tools:\n",
    "- Job Description: [List the required and prefered skills and tools needed for the job.]\n",
    "- Applicant: [Highlight the applicant's education and and skill set.]\n",
    "- Summary of Alignment: [Provide a summary of how well the applicant's skills match the job requirements.]\n",
    "- Alignment Score: X.0/10\n",
    "\n",
    "4. Values, perks, development opportunities, and Company Culture Alignment:\n",
    "- Job Description: [Highlight the company's values, perks, culture, and development opportunities.]\n",
    "- Applicant: [Describe the applicant's values and what they seek in company culture and benifits.]\n",
    "- Summary of Alignment: [Provide a summary of how well the applicant aligns with the company.]\n",
    "- Alignment Score: X.0/10\n",
    "\n",
    "Overall Alignment Score:\n",
    "[Overall Alignment Score Calculations Go Here] =  X.X/10\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "[Provide a brief conclusion summarizing the overall alignment between the applicant and the job, highlighting key strengths and areas where alignment is lacking. This should help gauge whether the applicant would be a good fit for the role.]\n",
    "\n",
    "Alignment Scores Summary:\n",
    "\n",
    "Company and Industry Alignment Score: X.0/10\n",
    "Job Role and Responsibilities vs. Applicant Experience Score: X.0/10\n",
    "Education, Technical Skills, and Tools: X.0/10\n",
    "Values, perks, development opportunities, and Company Culture Alignment: X.0/10\n",
    "Overall Alignment Score: X.X/10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matchy-9000: Automated Student-Job Profile Alignment\n",
    "\n",
    "This Python script is designed to process and align student profiles with job descriptions using an AI-based scoring mechanism. The alignment process is broken down into several stages, including token estimation, batch processing, and result aggregation. The following description outlines the key components and functions used in this script.\n",
    "\n",
    "### 1. **Loading and Combining DataFrames**\n",
    "   - **Function:** `load_and_combine_saved_dfs(timeout_dir, base_filename)`\n",
    "   - **Description:** This function loads all previously saved DataFrames from a specified directory and combines them into a single DataFrame. It is useful for recovering progress from temporary files in case of a timeout or interruption.\n",
    "\n",
    "### 2. **Restarting the Processing**\n",
    "   - **Function:** `restart_processing(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch)`\n",
    "   - **Description:** This function restarts the processing of profiles by loading previously saved progress and marking already processed rows. It is useful for resuming work after an interruption.\n",
    "\n",
    "### 3. **Saving Temporary DataFrame**\n",
    "   - **Function:** `save_temp_dataframe(dataframe, timeout_dir, base_filename)`\n",
    "   - **Description:** This function saves the processed rows of the DataFrame to a temporary file in case of a timeout. This helps preserve progress so that processing can be resumed from the last saved state.\n",
    "\n",
    "### 4. **Extracting Alignment Scores**\n",
    "   - **Function:** `get_alignment_score(alignment_text)`\n",
    "   - **Description:** This function extracts and returns the alignment scores from the alignment text generated by the assistant. It parses the alignment text to identify scores for different categories and compiles them into a dictionary.\n",
    "\n",
    "### 5. **Replenishing Rate and Token Limits**\n",
    "   - **Functions:** `replenish_rps()` and `replenish_tps()`\n",
    "   - **Description:** These functions continuously replenish the rate and token limit semaphores to allow more requests and tokens per second to be processed.\n",
    "\n",
    "### 6. **Sending and Receiving Data from the Assistant**\n",
    "   - **Function:** `matchy(formatted_profile, assistant_id, row)`\n",
    "   - **Description:** This function sends a formatted profile to the assistant for processing, handles rate limits and retries, and returns the alignment text and token usage.\n",
    "\n",
    "### 7. **Cleaning Text for Processing**\n",
    "   - **Function:** `clean_text(row)`\n",
    "   - **Description:** This function formats the text from a row of the DataFrame into a structured string that combines position and applicant profile information. The formatted text is used later for alignment scoring and analysis.\n",
    "\n",
    "### 8. **Processing a Single Profile**\n",
    "   - **Function:** `process_batch(row, assistant_id, id_column)`\n",
    "   - **Description:** This function processes a single profile by cleaning the text, sending it for alignment scoring, and then extracting and returning the relevant alignment scores and token usage.\n",
    "\n",
    "### 9. **Batch Processing of Profiles**\n",
    "   - **Function:** `send_and_recive_batch(dataframe, pbar, periodic_save_dir, base_filename, assistant_id, batch, batch_completed, num_in_batch)`\n",
    "   - **Description:** This function processes a batch of student profiles using concurrent threads, updates the DataFrame with the results, and handles periodic saving. It manages threading for batch processing and updates the progress bar as profiles are processed.\n",
    "\n",
    "### 10. **Preparing and Managing Batches**\n",
    "   - **Function:** `prep_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch)`\n",
    "   - **Description:** Prepares and processes a batch of student profiles by setting up countdown and timeout mechanisms. Manages the timing of batches and handles potential timeouts during batch processing.\n",
    "\n",
    "### 11. **Batch Handling and Processing**\n",
    "   - **Function:** `get_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, assistant_id, max_rpm)`\n",
    "   - **Description:** Divides the DataFrame into batches based on token limits and processes each batch. Ensures that the total tokens per minute and requests per minute do not exceed specified limits.\n",
    "\n",
    "### 12. **Starting the Comparison Process**\n",
    "   - **Function:** `start_comparisons(SP, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, assistant_id, max_rpm)`\n",
    "   - **Description:** Initiates the comparison process by calling the function that processes student profiles in batches.\n",
    "\n",
    "### 13. **Token Estimation for Profiles**\n",
    "   - **Functions:** `estimate_tokens(text, model)` and `calculate_profile_tokens(df, model)`\n",
    "   - **Description:** These functions estimate the number of tokens required for each student profile based on the text length, model, and additional estimated tokens for instructions and output.\n",
    "\n",
    "### 14. **Processing Time Estimation**\n",
    "   - **Function:** `estimate_processing_time(len_df, est_total_tokens, max_tokens_per_minute, batch_size)`\n",
    "   - **Description:** Estimates the total processing time required to compare student profiles based on the number of profiles, estimated tokens, and token limits per minute.\n",
    "\n",
    "### 15. **Finalizing the Results**\n",
    "   - **Steps:**\n",
    "     - Combine the finished output with any previously saved output.\n",
    "     - Save the final DataFrame to a specified location.\n",
    "     - Calculate the final cost and total time taken for processing.\n",
    "\n",
    "### 16. **Global Variables and Resource Management**\n",
    "   - **Description:** \n",
    "     - The script utilizes global variables to track total tokens used, request and token limits, and active threads.\n",
    "     - Threading is used to manage resource limits and ensure smooth batch processing.\n",
    "\n",
    "### 17. **Cost and Time Calculation**\n",
    "   - **Description:** The script calculates the final cost of processing based on the total tokens used and the elapsed time.\n",
    "\n",
    "### 18. **Output and Display**\n",
    "   - **Description:** The script displays the estimated processing time, tokens used, and final cost using HTML widgets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_saved_dfs(timeout_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Function Name: load_and_combine_saved_dfs\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Loads all previously saved DataFrames from the specified directory and combines them into a single DataFrame.\n",
    "    This function is useful for recovering progress from temporary files in case of a timeout or interruption.\n",
    "\n",
    "    Parameters:\n",
    "    - timeout_dir (str): Directory path where temporary DataFrame files are saved.\n",
    "    - base_filename (str): The base filename used for saving the DataFrames.\n",
    "\n",
    "    Return Value:\n",
    "    DataFrame: A combined DataFrame containing all the loaded and concatenated data. \n",
    "               If no files are found, an empty DataFrame is returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all parquet files that match the base filename pattern in the specified directory\n",
    "    all_files = glob.glob(os.path.join(timeout_dir, f\"{base_filename}_*.parquet\"))\n",
    "    \n",
    "    # Load each file into a DataFrame and store them in a list\n",
    "    df_list = [pd.read_parquet(file) for file in all_files]\n",
    "    \n",
    "    if df_list:\n",
    "        # Concatenate all DataFrames in the list, remove duplicates, and reset the index\n",
    "        combined_df = pd.concat(df_list, ignore_index=True).drop_duplicates()\n",
    "        print(f\"Combined {len(df_list)} saved DataFrames.\")\n",
    "    else:\n",
    "        # If no files are found, return an empty DataFrame\n",
    "        combined_df = pd.DataFrame()\n",
    "    \n",
    "    return combined_df  # Return the combined DataFrame\n",
    "\n",
    "\n",
    "def restart_processing(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch):\n",
    "    \"\"\"\n",
    "    Function Name: restart_processing\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Restarts the processing of profiles by loading previously saved progress, marking already processed rows, \n",
    "    and continuing with the remaining unprocessed profiles. This function is useful for resuming work \n",
    "    after a timeout or interruption.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (DataFrame): The main DataFrame containing all profiles to be processed.\n",
    "    - pbar (tqdm): The progress bar object to visualize the progress of the comparisons.\n",
    "    - timeout_dir (str): Directory path where temporary DataFrame files are saved.\n",
    "    - periodic_save_dir (str): Directory path for saving periodic progress.\n",
    "    - base_filename (str): The base filename used for saving the DataFrames.\n",
    "    - assistant_id (str): The ID of the assistant used for generating the alignment scores.\n",
    "    - batch (DataFrame): The batch of profiles to be processed.\n",
    "    - num_in_batch (int): The number of profiles in the current batch.\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and combine any saved DataFrames from the previous processing attempts\n",
    "    combined_df = load_and_combine_saved_dfs(timeout_dir, base_filename)\n",
    "    \n",
    "    id_column = 'match-id'  # The unique identifier for each profile-job match\n",
    "    processed_flag = 'comp_processed'  # Column to flag if the profile has been processed\n",
    "            \n",
    "    if not combined_df.empty:\n",
    "        # Mark rows as processed if they were already included in the combined DataFrame\n",
    "        dataframe.loc[dataframe[id_column].isin(combined_df[id_column]), processed_flag] = True\n",
    "        \n",
    "    # Identify the remaining unprocessed rows\n",
    "    remaining_df = dataframe[~dataframe[processed_flag]]\n",
    "\n",
    "    if remaining_df.empty:\n",
    "        print(\"No remaining rows to process. All data has been processed.\")\n",
    "    else:\n",
    "        print(f\"{len(combined_df)} row have been processed. {len(remaining_df)} profiles remaining.\")\n",
    "        # Continue processing the remaining profiles\n",
    "        prep_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch)\n",
    "\n",
    "\n",
    "def save_temp_dataframe(dataframe, timeout_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Function Name: save_temp_dataframe\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Saves the processed rows of the DataFrame to a temporary file in case of a timeout or interruption. \n",
    "    This function helps to preserve progress so that processing can be resumed from the last saved state.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (DataFrame): The main DataFrame containing all profiles, including those already processed.\n",
    "    - timeout_dir (str): Directory path where the temporary DataFrame files will be saved.\n",
    "    - base_filename (str): The base filename used for saving the DataFrames.\n",
    "\n",
    "    Return Value:\n",
    "    str: The path to the saved file if rows were saved, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the DataFrame to only include rows that have been processed\n",
    "    processed_dataframe = dataframe[dataframe['comp_processed'] == True]\n",
    "    num_processed = len(processed_dataframe)\n",
    "    \n",
    "    if not processed_dataframe.empty:\n",
    "        # Construct the file path for saving the processed rows\n",
    "        temp_path = os.path.join(timeout_dir, f\"{base_filename}_{num_processed}.parquet\")\n",
    "        # Save the processed rows to a parquet file\n",
    "        processed_dataframe.to_parquet(temp_path)\n",
    "        print(f\"Saved processed rows to {temp_path} due to timeout.\")\n",
    "        return temp_path  # Return the path to the saved file\n",
    "    else:\n",
    "        # If no rows were processed, skip saving\n",
    "        print(\"No rows to save; skipping save.\")\n",
    "        return None  # Return None as no file was saved\n",
    "\n",
    "\n",
    "def get_alignment_score(alignment_text):\n",
    "    \"\"\"\n",
    "    Function Name: get_alignment_score\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Extracts and returns the alignment scores from the alignment text generated by the assistant. \n",
    "    This function parses the alignment text to identify the scores for different categories \n",
    "    and compiles them into a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - alignment_text (str): The text containing the alignment score summary.\n",
    "\n",
    "    Return Value:\n",
    "    dict: A dictionary containing the extracted scores for each alignment category. \n",
    "          If extraction fails, returns a dictionary with an error message and the extracted text.\n",
    "\n",
    "    Example of Returned Dictionary:\n",
    "    {\n",
    "        'Company and Industry Alignment Score': 8,\n",
    "        'Job Role and Responsibilities vs. Applicant Experience Score': 7,\n",
    "        'Education, Technical Skills, and Tools': 9,\n",
    "        'Values, perks, development opportunities, and Company Culture Alignment': 8,\n",
    "        'Overall Alignment Score': 8\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}  # Initialize an empty dictionary to store the results\n",
    "    \n",
    "    # Extract the relevant section of the alignment text that contains the scores\n",
    "    alignment_section = alignment_text.split(\"Alignment Scores Summary:\")[-1].strip()\n",
    "    \n",
    "    # Extract the scores using a regular expression to find patterns like \"8.0/10\"\n",
    "    scores = re.findall(r'(\\d+\\.\\d+)/10', alignment_section)\n",
    "    \n",
    "    # Ensure all five scores are found before assigning them to the dictionary\n",
    "    if len(scores) == 5:\n",
    "        results = {\n",
    "            'Company and Industry Alignment Score': (float(scores[0])),\n",
    "            'Job Role and Responsibilities vs. Applicant Experience Score': (float(scores[1])),\n",
    "            'Education, Technical Skills, and Tools Score': (float(scores[2])),\n",
    "            'Values, perks, development opportunities, and Company Culture Alignment Score': (float(scores[3])),\n",
    "            'Overall Alignment Score': (float(scores[4]))\n",
    "        }\n",
    "    else:\n",
    "        # If the scores couldn't be extracted correctly, return an error with the extracted text\n",
    "        results = {'Error': 'Could Not Extract All Scores', 'Extracted Text': alignment_section}\n",
    "                    \n",
    "    return results  # Return the dictionary with the scores or the error message\n",
    "\n",
    "\n",
    "def replenish_rps():\n",
    "    \"\"\"\n",
    "    Function Name: replenish_rps\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Continuously replenishes the rate limit semaphore to allow more requests per second (RPS). \n",
    "    This function runs in a loop, waiting for the signal that requests have started and \n",
    "    then replenishing the semaphore periodically.\n",
    "    These values are set in the 'Threads for resource management' region.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        time.sleep(0.1)  # Short delay to reduce CPU usage\n",
    "        requests_started.wait()  # Wait until the request process starts\n",
    "        while True:\n",
    "            time.sleep(1)  # Wait before replenishing the rate limit\n",
    "            for _ in range(rps - rate_limit._value):\n",
    "                rate_limit.release()  # Release the semaphore to allow more requests\n",
    "\n",
    "\n",
    "def replenish_tps():\n",
    "    \"\"\"\n",
    "    Function Name: replenish_tps\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Continuously replenishes the token limit semaphore to allow more tokens per second (TPS) \n",
    "    to be processed. This function runs in a loop, waiting for the signal that requests \n",
    "    have started and then replenishing the semaphore periodically. \n",
    "    These values are set in the 'Threads for resource management' region. \n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        time.sleep(0.1)  # Short delay to reduce CPU usage\n",
    "        requests_started.wait()  # Wait until the request process starts\n",
    "        while True:\n",
    "            time.sleep(1)  # Wait before replenishing the token limit\n",
    "            tokens_to_add = tps - token_limit._value  # Calculate how many tokens to add\n",
    "            if tokens_to_add > 0:\n",
    "                token_limit.release(tokens_to_add)  # Replenish the semaphore with the required tokens\n",
    "\n",
    "\n",
    "def matchy(formatted_profile, assistant_id, row):\n",
    "    \"\"\"\n",
    "    Function Name: matchy\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Sends a formatted profile to the assistant for processing, handles rate limits and retries, \n",
    "    and returns the alignment text and token usage. This function manages the process of sending \n",
    "    requests to the assistant API and handles responses, including managing rate limits.\n",
    "    \n",
    "    Note: For most exceptions, a delay and a retry mechanism are implemented. The most common exception will be token and rate limit errors, which will be handled by waiting and retrying the request. \n",
    "    Other exceptions will be logged and may trigger a retry or raise an error as needed. Usually a retry works. \n",
    "\n",
    "    Parameters:\n",
    "    - formatted_profile (str): The text of the student profile and job description formatted for the assistant.\n",
    "    - assistant_id (str): The ID of the assistant used for generating the alignment scores.\n",
    "    - row (Series): A row from the DataFrame containing information about the job position and applicant, \n",
    "      including the number of tokens required.\n",
    "\n",
    "    Return Value:\n",
    "    Tuple: A tuple containing:\n",
    "        - alignment_text (str): The text output from the alignment scoring process.\n",
    "        - total_tokens_used_in_run (int): The total number of tokens used in the request.\n",
    "        - total_input_tokens_used_in_run (int): The number of input tokens used.\n",
    "        - total_output_tokens_used_in_run (int): The number of output tokens used.\n",
    "\n",
    "    Exceptions/Errors:\n",
    "    - RateLimitError: Handles rate limit errors and retries the request after waiting.\n",
    "    - Other Exceptions: Handles unexpected errors and retries or raises them as needed.\n",
    "    \"\"\"\n",
    "    \n",
    "    global total_tokens_used, total_input_tokens_used, total_output_tokens_used, threads_in_loop  # Global variables to track token usage and active threads\n",
    "    try:\n",
    "        # Acquire tokens needed for the request and respect rate limits\n",
    "        tokens_needed = row['TokenCount_comp']\n",
    "        token_limit.acquire(tokens_needed)\n",
    "        rate_limit.acquire()\n",
    "        \n",
    "        if not requests_started.is_set():\n",
    "            requests_started.set()  # Signal that requests have started\n",
    "            \n",
    "        while True:\n",
    "            try:\n",
    "                # Create a new thread and send the formatted profile as a message\n",
    "                thread = client.beta.threads.create()\n",
    "                message = client.beta.threads.messages.create(thread_id=thread.id, content=formatted_profile, role=\"user\")\n",
    "                run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=assistant_id)\n",
    "                break  # Exit loop if successful\n",
    "            except Exception as e:\n",
    "                # Handle rate limit exceeded error (HTTP 429)\n",
    "                if '429' in str(e):\n",
    "                    print(\"Rate limit exceeded. Retrying after a delay.\")\n",
    "                    print(\"Error:\", e)\n",
    "                    time.sleep(5)  # Wait before retrying\n",
    "                    continue\n",
    "                # Handle bad request error (HTTP 400)\n",
    "                elif '400' in str(e):\n",
    "                    print(e)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e  # Raise other exceptions\n",
    "                \n",
    "        with threads_lock:\n",
    "            threads_in_loop += 1  # Increment active threads count\n",
    "        \n",
    "        while True:\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                # Check the status of the run\n",
    "                run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "                if run_status.status == \"completed\":\n",
    "                        \n",
    "                    # Update the total tokens used globally\n",
    "                    total_input_tokens_used_in_run = run_status.usage.prompt_tokens\n",
    "                    total_input_tokens_used += total_input_tokens_used_in_run\n",
    "                    \n",
    "                    total_output_tokens_used_in_run = run_status.usage.completion_tokens\n",
    "                    total_output_tokens_used += total_output_tokens_used_in_run\n",
    "                    \n",
    "                    total_tokens_used_in_run = run_status.usage.total_tokens\n",
    "                    total_tokens_used += total_tokens_used_in_run\n",
    "                    \n",
    "                    break  # Exit loop when processing is complete\n",
    "\n",
    "                elif run_status.status == \"failed\":\n",
    "                    # Handle rate limit exceeded within the run\n",
    "                    if run_status.last_error.code == 'rate_limit_exceeded':\n",
    "                        match1 = re.search(r'in (\\d+\\.\\d+)s', run_status.last_error.message)\n",
    "                        match2 = re.search(r'in (\\d+)ms', run_status.last_error.message)\n",
    "                        if match1:\n",
    "                            wait_time = float(match1.group(1)) + 1\n",
    "                            print(run_status.last_error.message)\n",
    "                            print('#ERROR3#: Rate limit exceeded. Waiting for', wait_time, 'seconds.')\n",
    "                            with threads_lock:\n",
    "                                threads_in_loop -= 1\n",
    "                            time.sleep(wait_time)\n",
    "                            return matchy(formatted_profile, assistant_id, row)\n",
    "                        elif match2:\n",
    "                            wait_time = float(match2.group(1)) / 1000.0 + 1  # Convert milliseconds to seconds\n",
    "                            print(run_status.last_error.message)\n",
    "                            print('#ERROR3#: Rate limit exceeded. Waiting for', wait_time, 'seconds.')\n",
    "                            with threads_lock:\n",
    "                                threads_in_loop -= 1\n",
    "                            time.sleep(wait_time)\n",
    "                            return matchy(formatted_profile, assistant_id, row)\n",
    "                    # Handle a generic error message\n",
    "                    elif 'Sorry, something went wrong' in str(run_status.last_error):\n",
    "                        time.sleep(2)\n",
    "                        with threads_lock:\n",
    "                            threads_in_loop -= 1\n",
    "                        return matchy(formatted_profile, assistant_id, row)\n",
    "                    else:\n",
    "                        print(\"Run failed. Trying again\", run_status.last_error)\n",
    "                        with threads_lock:\n",
    "                            threads_in_loop -= 1\n",
    "                        return matchy(formatted_profile, assistant_id, row)   \n",
    "            except RateLimitError as e:\n",
    "                print('#ERROR2#: Rate limit exceeded.')\n",
    "                time.sleep(2)\n",
    "                with threads_lock:\n",
    "                    threads_in_loop -= 1\n",
    "                return matchy(formatted_profile, assistant_id, row)   \n",
    "            except Exception as e:\n",
    "                if 'rate_limit_exceeded' in str(e):\n",
    "                    print('#ERROR2#: Rate limit exceeded. waiting and retrying.')\n",
    "                    with threads_lock:\n",
    "                        threads_in_loop -= 1\n",
    "                    time.sleep(10)\n",
    "                    return matchy(formatted_profile, assistant_id, row)\n",
    "                else:\n",
    "                    print(\"Error:\", e)\n",
    "                    raise e  # Raise other exceptions\n",
    "                \n",
    "        with threads_lock:\n",
    "            threads_in_loop -= 1  # Decrement active threads count\n",
    "\n",
    "        # Retrieve and return the result message from the assistant\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        for message in reversed(messages.data):\n",
    "            role = message.role  \n",
    "            for content in message.content:\n",
    "                if content.type == 'text' and role == 'assistant':\n",
    "                    return content.text.value, total_tokens_used_in_run, total_input_tokens_used_in_run, total_output_tokens_used_in_run\n",
    "\n",
    "    except RateLimitError as e:\n",
    "        print('#ERROR1#: Rate limit exceeded.')\n",
    "        time.sleep(3)\n",
    "    except Exception as e:\n",
    "        if 'rate_limit_exceeded' in str(e):\n",
    "            time.sleep(3)\n",
    "            return matchy(formatted_profile, assistant_id, row)\n",
    "        else:\n",
    "            raise e  # Raise other exceptions if not related to rate limit\n",
    "\n",
    "\n",
    "def clean_text(row):\n",
    "    \"\"\"\n",
    "    Function Name: clean_text\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Formats the text from a row of the DataFrame into a structured string that combines \n",
    "    position and applicant profile information. This formatted text is used later for \n",
    "    alignment scoring and analysis. This is the exact format that gpt-4o will recive the information in.\n",
    "\n",
    "    Parameters:\n",
    "    - row (Series): A row from the DataFrame containing information about a job position \n",
    "      and the corresponding applicant.\n",
    "\n",
    "    Return Value:\n",
    "    str: A formatted string containing the combined information of the job position and applicant profile.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a formatted string containing job position details and applicant information\n",
    "    formatted_text = f\"\"\"\n",
    "    \n",
    "    -----------------------------------------------\n",
    "    Position Profile \n",
    "    -----------------------------------------------\n",
    "\n",
    "    Company Name: {row['pos_Company']}\n",
    "    Position Title: {row['pos_Name']}\n",
    "\n",
    "    Position Description:\n",
    "    -----------------------------------------------\n",
    "\n",
    "    {row['pos_Job_desc_text']}\n",
    "\n",
    "    Other Skills/Requirements/Preferences Summary\n",
    "    -----------------------------------------------\n",
    "\n",
    "    Other requirements/preferences: \n",
    "    {row['pos_Other requirements/preferences']}\n",
    "\n",
    "    Other Skills:\n",
    "    {row['pos_Other Skills']}\n",
    "\n",
    "    Position Summary:\n",
    "    {row['pos_External Position Summary']}\n",
    "    \n",
    "    Position Skill Summary:\n",
    "    {row['pos_Position_skill_summary']}\n",
    "    \n",
    "    ----------------------------------------\n",
    "    Applicant Profile:\n",
    "    ----------------------------------------\n",
    "\n",
    "    Applicant Name: {row['stu_Legal Name']}\n",
    "\n",
    "    Applicant Resume: \n",
    "    ----------------------------------------\n",
    "    {row['stu_Resume_text']}\n",
    "\n",
    "    Applicant Essays:\n",
    "    ----------------------------------------\n",
    "\n",
    "    Essay - Dream Companies\n",
    "    {row['stu_Essay - Dream Companies']}\n",
    "\n",
    "    Essay - Experience in field of Study\n",
    "    {row['stu_Essay - Experience in field of Study']}\n",
    "\n",
    "    Essay - Influence on interest in the field\n",
    "    {row['stu_Essay - Influence on interest in the field']}\n",
    "\n",
    "    Essay - Internship Career Goals\n",
    "    {row['stu_Essay - Internship Career Goals']}\n",
    "\n",
    "    Essay - Overall Career Goals\n",
    "    {row['stu_Essay - Overall Career Goals']}\n",
    "\n",
    "    Applicant Skill/Education Summary\n",
    "    ----------------------------------------\n",
    "\n",
    "    {row['stu_Position_skill_summary']}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return formatted_text  # Return the formatted string\n",
    "\n",
    "\n",
    "def process_batch(row, assistant_id, id_column):\n",
    "    \"\"\"\n",
    "    Function Name: process_batch\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Processes a single profile by cleaning the text, sending it for alignment scoring, \n",
    "    and then extracting and returning the relevant alignment scores and token usage.\n",
    "\n",
    "    Parameters:\n",
    "    - row (Series): A row from the DataFrame containing information about a job position \n",
    "      and the corresponding applicant.\n",
    "    - assistant_id (str): The ID of the assistant used for generating the alignment scores.\n",
    "    - id_column (str): The name of the column containing the unique identifier for each \n",
    "      profile-job match.\n",
    "\n",
    "    Return Value:\n",
    "    Tuple: A tuple containing:\n",
    "        - profile_id (str): The unique identifier of the profile-job match.\n",
    "        - alignment_text (str): The text output from the alignment scoring process.\n",
    "        - alignment_scores (dict): A dictionary containing the alignment scores.\n",
    "        - actual_tokens_used (int): The number of tokens used during the alignment process.\n",
    "        - actual_input_tokens_used (int): The number of input tokens used.\n",
    "        - actual_output_tokens_used (int): The number of output tokens used.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean the text from the row to prepare it for alignment scoring\n",
    "    cleaned_text = clean_text(row)\n",
    "    \n",
    "    # Send the cleaned text to the assistant for alignment scoring and receive the results\n",
    "    alignment_text, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used = matchy(cleaned_text, assistant_id, row)\n",
    "\n",
    "    # Extract alignment scores from the returned alignment text\n",
    "    alignment_scores = get_alignment_score(alignment_text)\n",
    "\n",
    "    # Return the profile's unique ID, alignment text, scores, and token usage information\n",
    "    return row[id_column], alignment_text, alignment_scores, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used\n",
    "\n",
    "\n",
    "def send_and_recive_batch(dataframe, pbar, periodic_save_dir, base_filename, assistant_id, batch, batch_completed, num_in_batch):\n",
    "    \"\"\"\n",
    "    Function Name: send_and_recive_batch\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Processes a batch of student profiles using concurrent threads, updates the DataFrame with the results, \n",
    "    and handles periodic saving. The function manages the threading for batch processing and updates the \n",
    "    progress bar as profiles are processed.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (DataFrame): The DataFrame containing student profiles and job descriptions.\n",
    "    - pbar (tqdm): The progress bar object to visualize the progress of the comparisons.\n",
    "    - periodic_save_dir (str): Directory path for saving periodic progress.\n",
    "    - base_filename (str): The base filename used for saving files.\n",
    "    - assistant_id (str): The ID of the assistant used for generating the alignment scores.\n",
    "    - batch (DataFrame): The batch of student profiles to be processed.\n",
    "    - batch_completed (threading.Event): Event to signal the completion of batch processing.\n",
    "    - num_in_batch (int): The number of profiles in the current batch.\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    global total_comparisons, num_comparissons  # Use global variables to track comparisons\n",
    "\n",
    "    id_column = 'match-id'  # Unique identifier for each profile-job match\n",
    "    ''' Note on the number of workers used in the ThreadPoolExecutor:\n",
    "    This is a very I/O bound task, so the number of workers can be set much higher than the number of CPU cores. \n",
    "    We keep the number of workers equal to the number of profiles in the batch to process them concurrently.\n",
    "    This also helps in utilizing the available tokens more efficiently and avoiding rate limits. \n",
    "    If rate limits become an issue, the number of workers can be reduced.\n",
    "    '''\n",
    "    workers = num_in_batch  # Number of worker threads to use, based on batch size.\n",
    "\n",
    "    # #region as_completed code\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit each row in the batch for processing\n",
    "        future_to_profile = {\n",
    "            executor.submit(process_batch, row, assistant_id, id_column): row\n",
    "            for idx, row in batch.iterrows()\n",
    "        }\n",
    "\n",
    "        # Process each completed future as it finishes\n",
    "        for future in as_completed(future_to_profile):\n",
    "            try:\n",
    "                # Retrieve the result from the future\n",
    "                result = future.result()\n",
    "                profile_id, alignment_text, alignment_scores, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used = result\n",
    "                \n",
    "                # #region updating the DataFrame with results\n",
    "                \n",
    "                # Mark the profile as processed\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'comp_processed'] = True\n",
    "                \n",
    "                # Extract and store the alignment scores from the result\n",
    "                overall_alignment_score = alignment_scores.get('Overall Alignment Score')\n",
    "                company_and_industry_alignment_score = alignment_scores.get('Company and Industry Alignment Score')\n",
    "                job_role_and_responsibilities_score = alignment_scores.get('Job Role and Responsibilities vs. Applicant Experience Score')\n",
    "                education_technical_skills_and_tools_score = alignment_scores.get('Education, Technical Skills, and Tools Score')\n",
    "                values_perks_development_culture_score = alignment_scores.get('Values, perks, development opportunities, and Company Culture Alignment Score')\n",
    "                \n",
    "                # Update the DataFrame with the alignment text and scores\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Alignment Text'] = alignment_text \n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Overall Alignment Score'] = overall_alignment_score\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Company and Industry Alignment Score'] = company_and_industry_alignment_score\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Job Role and Responsibilities vs. Applicant Experience Score'] = job_role_and_responsibilities_score\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Education, Technical Skills, and Tools Score'] = education_technical_skills_and_tools_score\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Values, perks, development opportunities, and Company Culture Alignment Score'] = values_perks_development_culture_score\n",
    "                \n",
    "                # Update the DataFrame with the actual tokens used in processing\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Comp Actual Tokens Used'] = actual_tokens_used\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Comp Actual Input Tokens Used'] = actual_input_tokens_used\n",
    "                dataframe.loc[dataframe[id_column] == profile_id, 'Comp Actual Output Tokens Used'] = actual_output_tokens_used     \n",
    "                \n",
    "                # Increment the comparison counter and periodically save the DataFrame\n",
    "                num_comparissons += 1\n",
    "                if num_comparissons % 50 == 0:\n",
    "                    path = f\"{periodic_save_dir}/{base_filename}_{num_comparissons}.parquet\"\n",
    "                    dataframe.to_parquet(path)\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Print any errors that occur during processing\n",
    "                print(\"Error processing profile:\", e)\n",
    "                continue\n",
    "    \n",
    "        # Signal that the batch processing is complete\n",
    "        batch_completed.set()\n",
    "\n",
    "\n",
    "def prep_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch):\n",
    "    \"\"\"\n",
    "    Function Name: prep_batch\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Prepares and processes a batch of student profiles by setting up countdown and timeout mechanisms. \n",
    "    Manages the timing of batches and handles potential timeouts during batch processing.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (DataFrame): The DataFrame containing student profiles and job descriptions.\n",
    "    - pbar (tqdm): The progress bar object to visualize the progress of the comparisons.\n",
    "    - timeout_dir (str): Directory path where temporary files will be saved in case of a timeout.\n",
    "    - periodic_save_dir (str): Directory path for saving periodic progress.\n",
    "    - base_filename (str): The base filename used for saving files.\n",
    "    - assistant_id (str): The ID of the assistant used for generating the alignment scores.\n",
    "    - batch (DataFrame): The batch of student profiles to be processed.\n",
    "    - num_in_batch (int): The number of profiles in the current batch.\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Event to signal when the countdown timer has finished\n",
    "    countdown_finished = threading.Event()\n",
    "\n",
    "    def countdown_timer(seconds):\n",
    "        \"\"\"Runs a countdown timer for 60 seconds. Required for keeping token usage per minute under the limits.\"\"\"\n",
    "        with tqdm(total=seconds, desc=f\"Processing {num_in_batch} rows. Time Till Next Batch:\", \n",
    "                  bar_format=\"{l_bar}{bar}| {remaining} seconds\", leave=False) as pbar:\n",
    "            for i in range(seconds):\n",
    "                time.sleep(1)\n",
    "                pbar.update(1)\n",
    "        countdown_finished.set()  # Signal that the countdown has finished\n",
    "\n",
    "    def timeout_handler():\n",
    "        \"\"\"Handles the situation where processing takes too long and triggers a timeout.\"\"\"\n",
    "        if countdown_finished.is_set():\n",
    "            save_temp_dataframe(dataframe, timeout_dir, base_filename)  # Save progress if the countdown is complete\n",
    "            print(\"Timeout occurred, saving current state and restarting process.\")\n",
    "            restart_processing(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch)\n",
    "        else:\n",
    "            print(\"Timeout occurred, but batch has already started processing. Ignoring timeout.\")\n",
    "\n",
    "    # Start the countdown timer in a separate thread\n",
    "    countdown_thread = threading.Thread(target=countdown_timer, args=(60,))\n",
    "    countdown_thread.start()\n",
    "\n",
    "    # Set up a timeout handler to trigger after 300 seconds if the batch hasn't completed\n",
    "    timeout_thread = threading.Timer(300, timeout_handler)\n",
    "    timeout_thread.start()\n",
    "    \n",
    "    try:\n",
    "        # Process the batch and wait for completion\n",
    "        send_and_recive_batch(dataframe, pbar, periodic_save_dir, base_filename, assistant_id, batch, batch_completed, num_in_batch)\n",
    "        countdown_finished.wait()  # Wait until the countdown finishes\n",
    "        batch_completed.wait()  # Wait until the batch processing completes\n",
    "    finally:\n",
    "        # Clean up threads and reset events\n",
    "        countdown_thread.join()  # Ensure the countdown thread finishes\n",
    "        timeout_thread.cancel()  # Cancel the timeout thread if it hasn't triggered\n",
    "        batch_completed.clear()  # Clear the batch completed event\n",
    "\n",
    "\n",
    "def get_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, assistant_id, max_rpm):\n",
    "    \"\"\"\n",
    "    Function Name: get_batch\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Divides the DataFrame into batches based on token limits and processes each batch. \n",
    "    Ensures that the total tokens per minute and requests per minute do not exceed specified limits.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (DataFrame): The DataFrame containing student profiles and job descriptions.\n",
    "    - pbar (tqdm): The progress bar object to visualize the progress of the comparisons.\n",
    "    - timeout_dir (str): Directory path where temporary files will be saved in case of a timeout.\n",
    "    - periodic_save_dir (str): Directory path for saving periodic progress.\n",
    "    - base_filename (str): The base filename used for saving files.\n",
    "    - max_tokens_per_minute (int): The maximum number of tokens that can be processed per minute.\n",
    "    - assistant_id (str): The ID of the assistant used for generating the alignment scores.\n",
    "    - max_rpm (int): The maximum number of requests per minute allowed.\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    id_column = dataframe['match-id']  # Identifier for each profile-job match\n",
    "    batch = pd.DataFrame(columns=dataframe.columns)  # Initialize an empty DataFrame for the batch\n",
    "    batch_token_count = 0  # Counter for the total tokens in the current batch\n",
    "    num_in_batch = 0  # Counter for the number of profiles in the current batch\n",
    "    \n",
    "    # Iterate over each row in the DataFrame to create batches\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        if row['comp_processed']:\n",
    "            continue  # Skip rows that have already been processed\n",
    "        \n",
    "        current_token_count = int(row['TokenCount_comp'])  # Get the token count for the current row\n",
    "\n",
    "        # Check if adding the current row would exceed the token or request limits\n",
    "        if batch_token_count + current_token_count > max_tokens_per_minute or num_in_batch == max_rpm:\n",
    "            # Process the current batch\n",
    "            prep_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch)\n",
    "\n",
    "            # Reset the batch and token counter for the next batch\n",
    "            batch = pd.DataFrame(columns=dataframe.columns)\n",
    "            batch_token_count = 0\n",
    "            num_in_batch = 0\n",
    "        \n",
    "        # Add the current row to the batch\n",
    "        if batch.empty:\n",
    "            batch = pd.DataFrame([row])\n",
    "        else:\n",
    "            batch = pd.concat([batch, pd.DataFrame([row])], ignore_index=True)\n",
    "            \n",
    "        batch_token_count += current_token_count  # Update the token count for the batch\n",
    "        num_in_batch += 1  # Increment the batch size\n",
    "    \n",
    "    # Process any remaining rows in the final batch\n",
    "    if not batch.empty:\n",
    "        prep_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, assistant_id, batch, num_in_batch)\n",
    "\n",
    "\n",
    "def start_comparisons(SP, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, assistant_id, max_rpm):\n",
    "    \"\"\"\n",
    "    Function Name: start_comparisons\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Initiates the comparison process by calling the function that processes student profiles in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - SP (DataFrame): The DataFrame containing student profiles and job descriptions.\n",
    "    - pbar (tqdm): The progress bar object to visualize the progress of the comparisons.\n",
    "    - timeout_dir (str): Directory path where temporary files will be saved in case of a timeout.\n",
    "    - periodic_save_dir (str): Directory path for saving periodic progress.\n",
    "    - base_filename (str): The base filename used for saving files.\n",
    "    - max_tokens_per_minute (int): The maximum number of tokens that can be processed per minute.\n",
    "    - assistant_id (str): The ID of the assistant used for generating the alignment scores.\n",
    "    - max_rpm (int): The maximum number of requests per minute allowed.\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    get_batch(SP, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, assistant_id, max_rpm)\n",
    "\n",
    "\n",
    "def estimate_tokens(text, model):\n",
    "    \"\"\"\n",
    "    Function Name: estimate_tokens\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Estimates the number of tokens in a given text based on the encoding model used.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text for which token estimation is required.\n",
    "    - model (str): The model used for encoding the text (e.g., 'gpt-4o-mini').\n",
    "\n",
    "    Return Value:\n",
    "    int: The number of tokens estimated in the input text.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)  # Get the encoding for the specified model\n",
    "    tokens = encoding.encode(text)  # Encode the text to determine the number of tokens\n",
    "    return len(tokens)  # Return the number of tokens\n",
    "\n",
    "\n",
    "def calculate_profile_tokens(df, model):\n",
    "    \"\"\"\n",
    "    Function Name: calculate_profile_tokens\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Calculates the estimated number of tokens required for each student profile by \n",
    "    considering the text length, model, and additional estimated tokens for instructions and output.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing student profiles and job descriptions.\n",
    "    - model (str): The model used for token estimation.\n",
    "\n",
    "    Return Value:\n",
    "    DataFrame: The updated DataFrame with estimated token counts for each profile.\n",
    "    \"\"\"\n",
    "    \n",
    "    id_column = 'match-id'  # Unique identifier for each profile-job match\n",
    "    \n",
    "    # Estimate token counts for instructions and average output\n",
    "    instructions_token_est = 1000  # Estimated tokens for instructions (Because each API call is on its own thread, instruction tokens are included in every call.)\n",
    "    avg_token_output =  700  # Estimated tokens for the output\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        formatted_text = clean_text(row)  # Clean and format the text for the profile\n",
    "        token_count = estimate_tokens(formatted_text, model)  # Estimate the number of tokens for the text\n",
    "        \n",
    "        # Update the DataFrame with the estimated token counts\n",
    "        df.loc[df[id_column] == row[id_column], 'TokenCount_comp'] = token_count + instructions_token_est + avg_token_output\n",
    "        df.loc[df[id_column] == row[id_column], 'TokenInputCount_comp'] = token_count + instructions_token_est\n",
    "        df.loc[df[id_column] == row[id_column], 'TokenOutputCount_comp'] = avg_token_output\n",
    "        \n",
    "    return df  # Return the updated DataFrame\n",
    "\n",
    "\n",
    "def estimate_processing_time(len_df, est_total_tokens, max_tokens_per_minute, batch_size):\n",
    "    \"\"\"\n",
    "    Function Name: estimate_processing_time\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Estimates the total processing time required to compare student profiles based on the number of profiles,\n",
    "    estimated tokens, and token limits per minute.\n",
    "\n",
    "    Parameters:\n",
    "    - len_df (int): The number of rows in the DataFrame (i.e., the number of profiles to process).\n",
    "    - est_total_tokens (int): The estimated total number of tokens required for processing all profiles.\n",
    "    - max_tokens_per_minute (int): The maximum number of tokens that can be processed per minute.\n",
    "    - batch_size (int): The number of profiles processed in each batch.\n",
    "\n",
    "    Return Value:\n",
    "    Tuple[int, int]: A tuple containing the estimated hours and minutes required for processing.\n",
    "    \"\"\"\n",
    "    # Calculate the number of batches required to process all profiles\n",
    "    num_batches = math.ceil(len_df / batch_size)\n",
    "    \n",
    "    # Determine the time required based on whether token limit or request limit is the bottleneck\n",
    "    if est_total_tokens / num_batches <= max_tokens_per_minute:\n",
    "        minutes_required = num_batches  # Limited by requests per minute\n",
    "    else:\n",
    "        minutes_required = est_total_tokens / max_tokens_per_minute  # Limited by tokens per minute\n",
    "    \n",
    "    minutes_required = math.ceil(minutes_required)  # Round up to the nearest minute\n",
    "    \n",
    "    # Convert total minutes to hours and minutes\n",
    "    est_hours = minutes_required // 60  # Calculate hours\n",
    "    est_minutes = minutes_required % 60  # Calculate remaining minutes\n",
    "    \n",
    "    return est_hours, est_minutes  # Return estimated processing time in hours and minutes\n",
    "\n",
    "\n",
    "# Define the model to use for generating alignment scores\n",
    "'''\n",
    "Note: ALWAYS USE THE 'gpt-4o-mini' MODEL IF YOU ARE ONLY TESTING THE CODE. \n",
    "It's 50x cheaper than the 'gpt-4o' model, and if you are running/modifying the code, you will likely need to run it over and over, and the output matters less. \n",
    "Switch to 'gpt-4o' if the primary focus is the output.\n",
    "'''\n",
    "model = 'gpt-4o-mini'  # Set the model to 'gpt-4o-mini' for testing or 'gpt-4o' for final runs.\n",
    "API_tier = 3  # Set the API tier based on your OpenAI account level.\n",
    "\n",
    "# Define the maximum number of requests per minute to avoid rate limiting\n",
    "max_rpm = 150\n",
    "\n",
    "# Start a timer to measure the total time taken for the entire process\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a new assistant using OpenAI's API with the specified model and instructions\n",
    "matchy_assistent = client.beta.assistants.create(\n",
    "    name=\"Student Profile Technical Skill and Education Extract-O-Bot\",  # Name of the assistant\n",
    "    instructions=instruction,  # Instructions for the assistant on how to perform the task\n",
    "    model=model,  # Model to be used for the task\n",
    "    temperature=1,  # Controls the creativity of the model's responses (1 means more creative)\n",
    "    top_p=1  # Controls the diversity of the model's responses (1 means maximum diversity)\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize columns in the DataFrame 'SP' to store alignment scores and text outputs\n",
    "#region alignment text and scores columns\n",
    "SP['Alignment Text'] = ''  # Column to store the alignment text generated by the assistant\n",
    "SP['Overall Alignment Score'] = 0.0  # Column to store the overall alignment score\n",
    "SP['Company and Industry Alignment Score'] = 0.0  # Column to store the company and industry alignment score\n",
    "SP['Job Role and Responsibilities vs. Applicant Experience Score'] = 0.0  # Column to store job role alignment score\n",
    "SP['Education, Technical Skills, and Tools Score'] = 0.0  # Column to store education and skills alignment score\n",
    "SP['Values, perks, development opportunities, and Company Culture Alignment Score'] = 0.0  # Column to store values and culture alignment score\n",
    "SP['match-id'] = SP['pos_(Do Not Modify) Job Posting'] + '_' + SP['stu_(Do Not Modify) Application']  # Unique identifier for each profile-job match\n",
    "SP['comp_processed'] = False  # Flag to indicate whether the profile has been processed or not\n",
    "#endregion\n",
    "\n",
    "# Estimate the maximum number of tokens that can be processed per minute based on the model used\n",
    "#region token and time estimation\n",
    "\n",
    "#region tokens per minute\n",
    "''' \n",
    "Note: The maximum number of tokens per minute varies depending on your OpenAI API tier and the model used.\n",
    "Info on tokens per minute for different models and API tiers can be found here: https://platform.openai.com/docs/guides/rate-limits\n",
    "Info on your specific API tier and rate limits can be found in the 'Your Profile' section under 'Limits' on the OpenAI platform.\n",
    "If you are using tier 1, doing a large number of comparisons will be slooowww (2+ hours for 1000 comparisons). You can only process about 4-5 rows a minute. \n",
    "Once you are tier 2 or higher, you can process 150 rows a minute and 1000 rows in 7 minutes.\n",
    "'''\n",
    "\n",
    "# Determine max tokens per minute based on the model and API tier\n",
    "if model == 'gpt-4o-mini':\n",
    "    if API_tier == 1:\n",
    "        max_tokens_per_minute = 200000  # Tier 1 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 2:\n",
    "        max_tokens_per_minute = 2000000  # Tier 2 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 3:\n",
    "        max_tokens_per_minute = 4000000  # Tier 3 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 4:\n",
    "        max_tokens_per_minute = 10000000  # Tier 4 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 5:\n",
    "        max_tokens_per_minute = 150000000  # Tier 5 limit for 'gpt-4o-mini'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported API tier for 'gpt-4o-mini'\")\n",
    "\n",
    "elif model == 'gpt-4o':\n",
    "    if API_tier == 1:\n",
    "        max_tokens_per_minute = 25000  # Tier 1 limit for 'gpt-4o'\n",
    "    elif API_tier == 2:\n",
    "        max_tokens_per_minute = 450000  # Tier 2 limit for 'gpt-4o'\n",
    "    elif API_tier == 3:\n",
    "        max_tokens_per_minute = 800000  # Tier 3 limit for 'gpt-4o'\n",
    "    elif API_tier == 4:\n",
    "        max_tokens_per_minute = 2000000  # Tier 4 limit for 'gpt-4o'\n",
    "    elif API_tier == 5:\n",
    "        max_tokens_per_minute = 30000000  # Tier 5 limit for 'gpt-4o'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported API tier for 'gpt-4o'\")\n",
    "\n",
    "else:\n",
    "    print('I only made code for gpt-4o and gpt-4o-mini. If you want to use a different model, you will have to add the token limits yourself.')\n",
    "    token_limit_input = input('Enter a token limit per minute for the model you are using: ')\n",
    "    \n",
    "    # Convert the input to an integer and set it as the max_tokens_per_minute\n",
    "    try:\n",
    "        max_tokens_per_minute = int(token_limit_input.replace(\",\", \"\").strip())  # Remove commas if present and convert to integer\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Invalid input. Please enter a valid integer for the token limit.\")\n",
    "\n",
    "#endregion\n",
    "\n",
    "# Initialize columns in the DataFrame 'SP' to track the actual number of tokens used\n",
    "#region Actual tokens used columns\n",
    "SP['Actual Tokens Used'] = 0  # Total number of tokens used\n",
    "SP['Actual Input Tokens Used'] = 0  # Number of input tokens used\n",
    "SP['Actual Output Tokens Used'] = 0  # Number of output tokens used\n",
    "#endregion\n",
    "\n",
    "# Initialize columns in the DataFrame 'SP' to estimate the number of tokens needed\n",
    "#region token count columns\n",
    "SP['TokenCount_comp'] = 0  # Estimated total number of tokens for each comparison\n",
    "SP['TokenInputCount_comp'] = 0  # Estimated number of input tokens for each comparison\n",
    "SP['TokenOutputCount_comp'] = 0  # Estimated number of output tokens for each comparison\n",
    "#endregion\n",
    "\n",
    "# Calculate the estimated number of tokens required for each profile-job comparison\n",
    "SP = calculate_profile_tokens(SP, model)\n",
    "\n",
    "# Estimate the total token usage and the cost associated with processing the profiles\n",
    "#region tokens use and price estimation\n",
    "\n",
    "# Define the relevant columns for token calculations\n",
    "token_cols = ['TokenCount_comp',  # Total token count for comparison\n",
    "              'TokenInputCount_comp',  # Input token count for comparison\n",
    "              'TokenOutputCount_comp',  # Output token count for comparison\n",
    "              ]\n",
    "\n",
    "# Calculate the total tokens required for all profiles\n",
    "token_subset = SP[token_cols]\n",
    "est_total_tokens = token_subset['TokenCount_comp'].sum()  # Sum of total tokens for all comparisons\n",
    "est_total_input_tokens = token_subset['TokenInputCount_comp'].sum()  # Sum of input tokens for all comparisons\n",
    "est_total_output_tokens = token_subset['TokenOutputCount_comp'].sum()  # Sum of output tokens for all comparisons\n",
    "\n",
    "# Compute the maximum token value for a single comparison\n",
    "max_token_value = SP['TokenCount_comp'].max()\n",
    "\n",
    "#region cost per token\n",
    "# Define the cost per token based on the model used\n",
    "if model == 'gpt-4o-mini':\n",
    "    price_per_input_token = 0.00000015  # Cost per input token for 'gpt-4o-mini'\n",
    "    price_per_output_token = 0.00000030  # Cost per output token for 'gpt-4o-mini'\n",
    "elif model == 'gpt-4o':\n",
    "    price_per_input_token = 0.000005  # Cost per input token for 'gpt-4o'\n",
    "    price_per_output_token = 0.000015  # Cost per output token for 'gpt-4o'\n",
    "\n",
    "# Estimate the total price for processing all profiles\n",
    "est_total_price = (est_total_input_tokens * price_per_input_token) + (est_total_output_tokens * price_per_output_token)\n",
    "#endregion\n",
    "\n",
    "#endregion\n",
    "\n",
    "# Estimate the total time required to process all profiles based on the token usage and batch size\n",
    "#region time estimation\n",
    "len_df = len(SP)  # Number of rows in the DataFrame (total profiles to be processed)\n",
    "est_hours, est_minutes = estimate_processing_time(len_df, est_total_tokens, max_tokens_per_minute, max_rpm)  # Estimated time in hours and minutes\n",
    "#endregion\n",
    "\n",
    "#region Display the token and time estimation results\n",
    "\n",
    "# Create HTML widgets to display the estimated processing details to the user\n",
    "total_comparisons_label = widgets.HTML(value=f\"<b>Total number of profiles to compare:</b> {len(SP)}\")  # Display the total number of profiles\n",
    "est_tokens_label = widgets.HTML(value=f\"<b>Estimate for total number of tokens required:</b> {est_total_tokens}\")  # Display estimated total tokens needed\n",
    "est_price_label = widgets.HTML(value=f\"<b>Estimate for total price for processing:</b> ${est_total_price:.2f}\")  # Display estimated total price\n",
    "est_time_label = widgets.HTML(value=f\"<b>Estimated time required:</b> {est_hours} hours and {est_minutes} minutes\")  # Display estimated time required\n",
    "\n",
    "# Display all the estimation results to the user\n",
    "display(total_comparisons_label, est_tokens_label, est_price_label, est_time_label)\n",
    "#endregion\n",
    "\n",
    "#endregion\n",
    "\n",
    "\n",
    "\n",
    "#region Global variables\n",
    "\n",
    "# Initialize global variables to track total tokens used in the process\n",
    "total_tokens_used = 0  # Track total tokens used for the entire process\n",
    "total_input_tokens_used = 0  # Track total input tokens used\n",
    "total_output_tokens_used = 0  # Track total output tokens used\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region Threads for resource management\n",
    "\n",
    "# Set the rate and token limits for processing\n",
    "''' \n",
    "Note: You will hit rate limit errors if you send too many API requests at the same time, so you have to lower the number of batches you send a second. \n",
    "This is handled by using semaphores in functions about matchy. Adjust these to higher per second values if you are using a higher tier. \n",
    "They are set to the lowest possible values for tier 1.\n",
    "'''\n",
    "rps = 5  # Requests per second\n",
    "tps = 20000 # Tokens per second (Don't lower this beyond 10k or else some rows with a higher token count will not be processed)\n",
    "\n",
    "# Create a threading lock to manage access to shared resources\n",
    "threads_lock = threading.Lock()\n",
    "\n",
    "# Create semaphores to control the rate and token limits\n",
    "rate_limit = threading.Semaphore(rps)  # Semaphore for rate limiting\n",
    "token_limit = threading.Semaphore(tps)  # Semaphore for token limiting\n",
    "\n",
    "# Start threads to replenish the rate and token limits continuously\n",
    "threading.Thread(target=replenish_rps, daemon=True).start()  # Thread to replenish requests per second\n",
    "threading.Thread(target=replenish_tps, daemon=True).start()  # Thread to replenish tokens per second\n",
    "\n",
    "# Create threading events to synchronize batch completion and request initiation\n",
    "batch_completed = threading.Event()  # Event to signal batch completion\n",
    "requests_started = threading.Event()  # Event to signal the start of requests\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region thread monitoring\n",
    "\n",
    "# Initialize a counter to track the number of threads in the loop\n",
    "threads_in_loop = 0\n",
    "\n",
    "# Optional code to monitor the number of active threads waiting for API call completions, currently commented out\n",
    "# def monitor_loop_threads():\n",
    "#     requests_started.wait()\n",
    "#     while True:\n",
    "#         with threads_lock:\n",
    "#             print(f\"Threads in loop: {threads_in_loop}\")\n",
    "#         time.sleep(5)  # Print the number of threads in the loop every 5 seconds\n",
    "\n",
    "# # Start the loop monitoring in a background thread\n",
    "# threading.Thread(target=monitor_loop_threads, daemon=True).start()\n",
    "\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region define path for saving progress\n",
    "\n",
    "# Define paths for saving progress and final results\n",
    "''' \n",
    "Note: Sometimes an API request can get stuck, so I've added a reset process. You will need directories for periodic saves, timeouts, and the final save. \n",
    "'''\n",
    "timeout_dir = f'{path_to_project}/data/matchy_saves/timeout_dir'  # Directory for saving progress in case of timeout\n",
    "periodic_save_dir = f'{path_to_project}/data/matchy_saves/periodic_save_dir'  # Directory for periodic saves\n",
    "final_save_path = f'{path_to_project}/data/matchy_saves/final_save_dir/matchy_final_save.parquet'  # Final save path for processed data\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region counter for progress bar\n",
    "\n",
    "# Initialize counters for the progress bar\n",
    "total_comparisons = len(SP)  # Total number of profiles to compare\n",
    "num_comparissons = 0  # Counter to track the number of comparisons made\n",
    "#endregion\n",
    "\n",
    "\n",
    "\n",
    "# Display the progress bar and start the comparison process\n",
    "with tqdm(total=total_comparisons, desc=\"Comparing Profiles\") as pbar:\n",
    "    start_comparisons(SP, pbar, timeout_dir, periodic_save_dir, 'SP_extraction_save', max_tokens_per_minute, matchy_assistent.id, max_rpm)\n",
    "\n",
    "\n",
    "#region final save\n",
    "\n",
    "# Combine the finished output with any previously saved output\n",
    "combined_df = load_and_combine_saved_dfs(timeout_dir, 'SP_matchy_save')\n",
    "\n",
    "# Save the final DataFrame if no previous saves exist\n",
    "if combined_df.empty:\n",
    "    SP.to_parquet(final_save_path)   \n",
    "else: \n",
    "    # Combine the new data with previously saved data and save the final DataFrame\n",
    "    final_df = pd.concat([SP, combined_df], ignore_index=True).drop_duplicates()\n",
    "    final_df.to_parquet(final_save_path)\n",
    "    \n",
    "#endregion\n",
    "    \n",
    "\n",
    "#region Final token and time calculation\n",
    "\n",
    "# Calculate the final cost and total time taken for processing\n",
    "total_price = (total_input_tokens_used * price_per_input_token) + (total_output_tokens_used * price_per_output_token)  # Calculate total price for processing\n",
    "end_time = time.time()  # Record the end time\n",
    "elapsed_time = end_time - start_time  # Calculate the total elapsed time\n",
    "\n",
    "# Convert the elapsed time into hours, minutes, and seconds for display\n",
    "elapsed_hours = int(elapsed_time // 3600)\n",
    "elapsed_minutes = int((elapsed_time % 3600) // 60)\n",
    "elapsed_seconds = int(elapsed_time % 60)\n",
    "formatted_time = f\"{elapsed_hours}h {elapsed_minutes}m {elapsed_seconds}s\"  # Format the elapsed time\n",
    "\n",
    "# Create HTML widgets to display the final processing results\n",
    "process_complete_label = widgets.HTML(value=f\"<b>Processing complete. Compared {total_comparisons} profiles. Final DataFrame saved to:</b> {final_save_path}\")\n",
    "tokens_label = widgets.HTML(value=f\"<b>Total tokens used:</b> {total_tokens_used}\")  # Display total tokens used\n",
    "price_label = widgets.HTML(value=f\"<b>Total price for processing:</b> ${total_price:.2f}\")  # Display total price for processing\n",
    "time_label = widgets.HTML(value=f\"<b>Total time used:</b> {formatted_time}\")  # Display total time taken\n",
    "\n",
    "#endregion\n",
    "\n",
    "\n",
    "# Display the final results to the user\n",
    "display(tokens_label, price_label, time_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results\n",
    "\n",
    "We now have alignment scores for each student-job pair in the DataFrame `SP`. These scores will be utilized in the next pipeline to generate a final ranking of students for each job position. Below, we can review some of the results produced by the Matchy-9000 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a path to a df containing alignment scores for each student-job pair from 2024 using gpt-4o-mini. If you generated your own, you can comment this out and use that instead.\n",
    "SP_path = f'{path_to_project}/data/SP_table/SP5_post_matchy.parquet'\n",
    "SP = pd.read_parquet(SP_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a list of Companies from 2024. If you'd like to see the matches for a specific company, copy and paste the name in the 'Company_Name' variable below.\n",
    "\n",
    "for company in SP['pos_Company'].unique():\n",
    "    print(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ''' \n",
    "stu_Legal Name\n",
    "pos_Name\n",
    "pos_Company\n",
    "Alignment Text\n",
    "Overall Alignment Score\n",
    "Company and Industry Alignment Score\n",
    "Job Role and Responsibilities vs. Applicant Experience Score\n",
    "Education, Technical Skills, and Tools Score\n",
    "Values, perks, development opportunities, and Company Culture Alignment Score\n",
    "'''\n",
    "\n",
    "cols = as_list(cols)\n",
    "\n",
    "alignment_scores = SP[cols]\n",
    "\n",
    "Company_Name = 'Media Group'\n",
    "\n",
    "alignment_scores = (alignment_scores[alignment_scores['pos_Company'] == Company_Name])\n",
    "\n",
    "# sort alignment scores by overall alignment score\n",
    "alignment_scores = alignment_scores.sort_values(by='Overall Alignment Score', ascending=False)\n",
    "\n",
    "pretty_print(alignment_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
