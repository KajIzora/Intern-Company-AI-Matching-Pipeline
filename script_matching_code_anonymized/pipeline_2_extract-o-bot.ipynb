{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipeline Stage 2: Extract-O-Bot</h2>\n",
    "\n",
    "<p>Welcome to stage 2 of our pipeline! After filtering out ineligible candidates, the second step in our pipeline is to extract keywords from student applications and position profiles. We will be using the OpenAI API to generate keywords from the text data. Our text data includes any unstructured text from the student applications and position profiles. This encompasses student resumes, student essays, job descriptions, external position summaries, and any free text input added to student or position profiles.</p>\n",
    "\n",
    "\n",
    "<p>Below the Extract-O-Bot code, you will find previously generated results that you can review without needing to run Extract-O-Bot.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~\n",
    "import re  # For regular expressions\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait # For multithreading\n",
    "import os  # For os.path operations\n",
    "import time  # For time-related functions\n",
    "import glob  # For file matching\n",
    "import pandas as pd  # For data manipulation\n",
    "from tqdm.notebook import tqdm  # For progress bar\n",
    "import threading  # For threading operations\n",
    "import tiktoken  # For token estimation\n",
    "import ipywidgets as widgets  # For displaying widgets\n",
    "from openai import OpenAI, RateLimitError, AzureOpenAI # For OpenAI API\n",
    "from mypackage.utils import *  # For utility functions\n",
    "import json\n",
    "\n",
    "# Error handling\n",
    "class RateLimitError(Exception):\n",
    "    pass\n",
    "class TimeoutException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the project path\n",
    "path_to_project = load_project_path()\n",
    "\n",
    "if path_to_project:\n",
    "    print(f\"Project path loaded: {path_to_project}\")\n",
    "else:\n",
    "    print(\"Please set the project path in the initial notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>API Key Setup</h3>\n",
    "\n",
    "<p>Below, you need to add your own API key. You can access these via Azure or OpenAI. Please note that they do cost money to use.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API key setup\n",
    "\n",
    "# client = AzureOpenAI(\n",
    "#     azure_endpoint = 'endpoint_goes_here',\n",
    "#     api_key= 'key_goes_here',\n",
    "#     api_version=\"2024-05-01-preview\"\n",
    "# )\n",
    "\n",
    "# My API key\n",
    "client = OpenAI(api_key='Your API key goes here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data. This is the table from pipeline 1. \n",
    "\n",
    "path = f'{path_to_project}/data/SP_table/SP1.parquet'\n",
    "\n",
    "\n",
    "# Load the data\n",
    "SP = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SP.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setting the Number of Student Applications and Job Postings for Testing</h3>\n",
    "\n",
    "<p>I would recommend using a subset of the data for testing purposes. You can set the number of student applications and job postings you want to use for testing.</p>\n",
    "\n",
    "<p>The number of rows processed will be <code>(num_job_postings * 2) + (num_student_applications * 2)</code> because we are using the student applications and job postings twice.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_student_applications = 25\n",
    "# num_job_postings = 25\n",
    "\n",
    "# #region Using subset of SP for testing\n",
    "\n",
    "# # take a random sample of 4 rows from SP\n",
    "# # SP = SP.sample(4, random_state=40)\n",
    "\n",
    "# # create a pos_grouped df with just the pos_(Do Not Modify) Job Posting column, pos_grouped by pos_(Do Not Modify) Job Posting\n",
    "# pos_grouped = SP['pos_(Do Not Modify) Job Posting']\n",
    "\n",
    "# stu_grouped = SP['stu_(Do Not Modify) Application']\n",
    "\n",
    "# # edit the pos_grouped df to only unique values\n",
    "# pos_grouped = pos_grouped.drop_duplicates()\n",
    "\n",
    "# # edit the stu_grouped df to only unique values\n",
    "# stu_grouped = stu_grouped.drop_duplicates()\n",
    "\n",
    "# # Randomly select 50 unique job postings from the pos_grouped df\n",
    "# sampled_job_postings = pos_grouped.sample(num_student_applications , random_state=30)\n",
    "\n",
    "# # Randomly select 50 unique applicants from the stu_grouped df\n",
    "# sampled_applicants = stu_grouped.sample(num_job_postings, random_state=30)\n",
    "\n",
    "# # edit SP to only rows where the pos_(Do Not Modify) Job Posting is in the sampled_job_postings\n",
    "# SP = SP[SP['pos_(Do Not Modify) Job Posting'].isin(sampled_job_postings)]\n",
    "\n",
    "# # edit SP to only rows where the stu_(Do Not Modify) Application is in the sampled_applicants\n",
    "# SP = SP[SP['stu_(Do Not Modify) Application'].isin(sampled_applicants)]\n",
    "\n",
    "# print('Shape')\n",
    "# print(SP.shape)\n",
    "\n",
    "# print('Unique Job Postings')\n",
    "# print(SP['pos_(Do Not Modify) Job Posting'].nunique())\n",
    "\n",
    "# print('Unique Applicants')\n",
    "# print(SP['stu_(Do Not Modify) Application'].nunique())\n",
    "\n",
    "# #endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Instructions for Extraction</h3>\n",
    "\n",
    "<p>Below are all the prompts I wrote for the keyword extraction. This is where you will have the most control over the results.</p>\n",
    "\n",
    "<p>The specific instructions I set are just something I came up with on the fly. I encourage everyone who tries this to create their own set of instructions to see what kind of results you get and how flexible the extraction can be.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stu technical skill and education extraction instructions\n",
    "S_TS_Ed_EOB_instructions = '''\n",
    "---\n",
    "\n",
    "Task Description:  \n",
    "You are an AI designed to extract and label technical skill and educational background keywords from job application profiles. Your objective is to identify and categorize the applicant's technical skills and educational background. Each technical skill should be labeled with a Relevance Score and a Skill Level Score.\n",
    "\n",
    "---\n",
    "\n",
    "Input Information:  \n",
    "You will receive an application profile that includes the following sections:  \n",
    "- Applicant Resume\n",
    "- Applicant Essays \n",
    "- Education Summary\n",
    "\n",
    "---\n",
    "\n",
    "Extraction Criteria:\n",
    "\n",
    "1. Technical Skills:\n",
    "    - Identify keywords that represent the technical skills possessed by the applicant.\n",
    "    - Each skill should be on its own line. For example, if the application states 'Qualitative & Quantitative Research Skills', Qualitative Research should be on one line, and Quantitative Research should be on another. \n",
    "    - Separate skills. If 'Data Analysis (pandas, NumPy, scikit-learn)' is in the application profile, Data Analysis, pandas, NumPy, and scikit-learn should all be on separate lines. \n",
    "    - Ensure ALL technical skills are extracted from the applicant's profile. \n",
    "    - Do not include soft skills when doing technical skill extraction.\n",
    "   \n",
    "\n",
    "2. Education:\n",
    "    - Identify keywords that represent the applicant's educational background.\n",
    "    - Don't only include the degree or major but also any relevant courses or academic experiences that are mentioned in the profile.\n",
    "\n",
    "---\n",
    "\n",
    "Labeling Criteria:\n",
    "\n",
    "1. Technical Skills:\n",
    "    - Consider the entire profile’s context when labeling the keywords. For instance, if \"statistics\" is extracted from the Skill/Education Summary, and the Resume shows extensive experience/interest in statistics, this context should be reflected in the Relevance and Skill Level Scores.\n",
    "    - Each keyword should have 2 labels related to a Relevance Score and a Skill Level Score.\n",
    "    - Relevance Score: \n",
    "        - The relevance score evaluates how well a technical skill aligns with the applicant’s field of study or industry of interest. To determine this, consider the applicant’s essays, work experience, and other relevant information in their profile.\n",
    "        - Labels:\n",
    "            - Low: The skill is not directly related to the applicant's experience, career goals, or industry of interest, or the skill is only mentioned one time with little to no context.\n",
    "            - Medium: The skill is somewhat related to the applicant's experience, career goals, or industry of interest, but the applicant has limited experience or demonstrates a vague interest in using the skill.\n",
    "            - High: The skill is directly related to the applicant's experience, career goals, or industry of interest, and the applicant has relevant experience or demonstrates a clear interest in using the skill.\n",
    "    - Skill Level Score:\n",
    "        - The skill level score evaluates the applicant’s proficiency in each technical skill.  \n",
    "        - If not explicitly stated, assume the expected proficiency level of a skill based on the context of the application profile. If it's still unclear, default to Intermediate.\n",
    "        - Labels:\n",
    "            - Beginner: The applicant has basic knowledge or limited experience with the skill.\n",
    "            - Intermediate: The applicant has a moderate level of knowledge or experience with the skill.\n",
    "            - Advanced: The applicant has an advanced level of knowledge or extensive experience with the skill.\n",
    "            \n",
    "2. Education:\n",
    "   - No labeling required for education keywords. Simply extract the relevant keywords.\n",
    "        \n",
    "---\n",
    "\n",
    "Output Criteria:\n",
    "\n",
    "- It is CRITICAL that you adhere strictly to the format provided below.\n",
    "- Output should be in plain text without any special formatting (e.g., HTML, Markdown, LaTeX).\n",
    "- Do not format your output with lists using hyphens or bullet points.\n",
    "- Do not use any bold text (i.e. **bold**) or formatted headers.\n",
    "- Do not add any explanations or extra text beyond the required output format.\n",
    "- Do not include any additional explanations or words in the output beyond what is outlined under 'Expected Output Format:' in these instructions.\n",
    "\n",
    "---\n",
    "\n",
    "Expected Output Format:\n",
    "\n",
    "Technical Skills:  \n",
    "[Technical Skill Keyword], (Relevance Score: [Insert Relevance Score Here]), (Skill Level Score: [Insert Skill Level Label Here])\n",
    "\n",
    "Education:\n",
    "[Education Keyword]\n",
    "\n",
    "---\n",
    "'''\n",
    "\n",
    "# stu Industry of Interest, Soft Skills, and Values/Motivations/Mission extraction instructions\n",
    "S_In_SS_V_EOB_instructions = '''\n",
    "---\n",
    "\n",
    "Task Description:  \n",
    "You are an AI designed to extract and label keywords from job application profiles related to the applicant's industry of interest, soft skills, and values/motivations/mission. \n",
    "\n",
    "---\n",
    "\n",
    "Input Information:  \n",
    "You will receive an application profile that includes the following sections:  \n",
    "- Applicant Resume\n",
    "- Applicant Essays \n",
    "- Applicant Education Summary\n",
    "\n",
    "---\n",
    "\n",
    "Extraction Criteria:\n",
    "\n",
    "1. Industries of Interest:\n",
    "   - Identify keywords that represent the industries the applicant is interested in.\n",
    "   - Utilize information related to the applicant's field of study, career goals, work experience, and any other relevant details in the profile to extract these keywords.\n",
    "   - In addition to directly extracting keywords from the document, infer and create keywords that likely correspond to the applicant's industry of interest based on the context of the application. For instance, if the applicant does not explicitly state \"Business Analytics\" but demonstrates significant interest in analyzing business trends, has taken coursework related to Business Analytics, and shows a likely inclination toward this field, then \"Business Analytics\" should be added as a keyword.\n",
    "   - Ensure ALL industries of interest are extracted or inferred from the applicant's profile, reflecting a comprehensive understanding of the applicant's potential career directions.\n",
    "\n",
    "2. Soft Skills:\n",
    "    - Identify keywords that represent the soft skills possessed by the applicant.\n",
    "    - Each skill should be on its own line. For example, if the application profile states 'Strong communication & organizational skills', Strong communication should be on one line, and Organizational skills should be on another.\n",
    "    - Separate skills. If 'Team player (collaboration, communication)' is in the application profile, Team player, collaboration, and communication should all be on separate lines.\n",
    "    - Ensure ALL soft skills are extracted from the applicant's profile.\n",
    "\n",
    "3. Values, Motivations, and Mission:\n",
    "    - Identify keywords that represent the applicant's values, motivations, and mission.\n",
    "\n",
    "---\n",
    "\n",
    "Labeling Criteria\n",
    "\n",
    "1. Industries of Interest:\n",
    "   - Each industry of interest keyword should be labeled with an \"Industry of Interest Score.\"\n",
    "   - Industry of Interest Score:\n",
    "      - The Industry of Interest Score measures how closely an industry keyword aligns with the applicant's academic background, work experience, and career goals.\n",
    "      - To determine the score, consider the overall context of the applicant's profile, including their essays, resume, and any relevant information in the Skill/Education Summary.\n",
    "      - Labels:\n",
    "         - Primary: This score is assigned to industries that are directly related to the applicant’s primary field of study or career aspirations. These are the main industries the applicant is focused on and has substantial experience or demonstrated interest in.\n",
    "         - Secondary: This score is assigned to industries that are closely related to the applicant’s primary industry of interest. These industries may be a natural extension or complementary field where the applicant has some experience or interest.\n",
    "         - Tertiary: This score is assigned to industries that are less related to the applicant’s main interests or experiences. These may be industries the applicant has mentioned in passing or has limited experience in.\n",
    "      - If the industry of interest is not explicitly stated in the profile, infer the appropriate score based on the context provided by the applicant's experience and stated goals. If the relevance is still unclear, default to labeling it as Secondary.\n",
    "\n",
    "\n",
    "2. Soft Skills:\n",
    "   - Each soft skill keyword should be labeled with a \"Skill Level Score.\"\n",
    "   - Skill Level Score:\n",
    "      - The Skill Level Score measures the applicant's proficiency or expertise in each soft skill.\n",
    "      - To determine the score, consider the full context of the applicant's profile, including their resume, essays, and any relevant information in the Skill/Education Summary. For example:\n",
    "         - Beginner: This score is assigned to soft skills where the applicant has basic knowledge or limited experience. The applicant may have recently started developing this skill or has only had minimal exposure to it in academic or work settings.\n",
    "         - Intermediate: This score is assigned to soft skills where the applicant has a moderate level of proficiency. The applicant has demonstrated a working knowledge of the skill and has applied it in various contexts, but may not yet be highly proficient.\n",
    "         - Advanced: This score is assigned to soft skills where the applicant has extensive experience and a high level of proficiency. The applicant has a deep understanding of the skill, often demonstrated through significant accomplishments, leadership, or repeated successful application in complex situations.\n",
    "   - If the proficiency level of a skill is not explicitly stated in the profile, infer the appropriate score based on the overall context provided by the applicant's experience and the nature of the skill. If the level of proficiency is still unclear, default to labeling it as Intermediate.\n",
    "\n",
    "\n",
    "3. Values, Motivations, and Mission:\n",
    "    - No labeling is necessary for keywords in this category. Simply extract the relevant keywords.\n",
    "\n",
    "---\n",
    "\n",
    "Output Criteria:\n",
    "\n",
    "- It is CRITICAL that you adhere strictly to the format provided below.\n",
    "- Output should be in plain text without any special formatting (e.g., HTML, Markdown, LaTeX).\n",
    "- Do not format your output with lists using hyphens or bullet points.\n",
    "- Do not use any bold text (i.e. **bold**) or formatted headers.\n",
    "- Do not add any explanations or extra text beyond the required output format.\n",
    "- Do not include any additional explanations or words in the output beyond what is outlined under 'Expected Output Format:' in these instructions.\n",
    "\n",
    "---\n",
    "\n",
    "Expected Output Format:\n",
    "\n",
    "Industries of Interest:\n",
    "[Industry Keyword], (Industry Label: [Insert Industry Label Here])\n",
    "\n",
    "Soft Skills:  \n",
    "[Soft Skill Keyword], (Skill Level Score: [Insert Skill Level Label Here])\n",
    "\n",
    "Values, Motivations, and Mission:\n",
    "[Value Keyword]\n",
    "\n",
    "---\n",
    "'''\n",
    "\n",
    "# pos technical skill extraction instructions\n",
    "P_TS_EOB_instructions = ''' \n",
    "---\n",
    "\n",
    "Task Description:  \n",
    "You are an AI designed to extract and label technical skill keywords from job position profiles. The profiles contain information related to a particular job position. Your objective is to identify and categorize technical skills possessed by the applicant. Each technical skill should be labeled with a Relevance Score and a Skill Level Score.\n",
    "\n",
    "---\n",
    "\n",
    "Input Information:  \n",
    "You will receive a job position profile that includes the following sections:  \n",
    "1. Company Name  \n",
    "2. Position Title  \n",
    "3. Position Description\n",
    "4. Other Skill/Requirements/Preferences Summary\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Extraction Criteria:\n",
    "    - Identify keywords that represent the technical skills possessed by the applicant.\n",
    "    - If the Position Title includes relevant technical skills, extract them as keywords.\n",
    "    - Treat the Position Description and Other Skill/Requirements/Preferences Summary sections as distinct entities. Extract keywords separately from each section. However, do not duplicate keywords.\n",
    "    - Each skill should be on its own line. For example, if the application states 'Qualitative & Quantitative Research Skills', Qualitative Research should be on one line, and Quantitative Research should be on another. \n",
    "    - Seperate skills. If 'Data Analysis (pandas, NumPy, scikit-learn)' is in the application profile, Data Analysis, pandas, NumPy and scikit-learn should all be on seperate lines. \n",
    "    - include education requirements/preferences in the extraction. For example, if the job position requires a Bachelor's degree in Computer Science, extract 'Computer Science' as a technical skill. If a job position states 'We are looking for candidates with a background in Finance or Economics,' extract 'Finance' and 'Economics' as technical skills.\n",
    "    - include experience requirements/preferences in the extraction. For example, if the position states 'We are looking for candidates with experience in business analytics,' extract 'Business Analytics' as a technical skill.\n",
    "    - Ensure ALL technical skills are extracted from the applicant's profile. \n",
    "\n",
    "---\n",
    "\n",
    "Labeling Criteria:\n",
    "- Consider the entire profile’s context when labeling the keywords. For instance, if \"statistics\" is extracted from the Position Summary, and the Position Description states that the applicant should have experience in statistics, this context should be reflected in the Necessity and Skill Level Scores.\n",
    "- Each keyword should have 2 labels related to a Necessity Score and a Skill Level Score.\n",
    "- Necessity Score:\n",
    "    - The necessary score evaluates how important a technical skill is to the job position. \n",
    "    - If not explicitly stated, assume the necessity of skills based on the context of the job position profile. If it's still unclear, default to Medium.\n",
    "    - Labels:\n",
    "        - High: The skill is a critical requirement for the job position.\n",
    "        - Medium: The skill is a preferred requirement for the job position.\n",
    "        - Low: The skill is a nice addition but not a requirement for the job position.\n",
    "- Skill Level Score: \n",
    "    - The skill level score evaluates the expected proficiency level of each technical skill.\n",
    "    - If not explicitly stated, assume the expected proficiency level of a skill based on the context of the job position profile. If it's stillunclear, default to Intermediate.\n",
    "       - Beginner: Applicants are expected to have basic knowledge or limited experience with the skill.\n",
    "       - Intermediate: Applicants are expected to have a moderate level of knowledge or experience with the skill.\n",
    "       - Advanced: Applicants are expected to have an advanced level of knowledge or extensive experience with the skill.\n",
    "        \n",
    "---\n",
    "\n",
    "Output Criteria:\n",
    "\n",
    "- It is CRITICAL that you adhere strictly to the format provided below.\n",
    "- Output should be in plain text without any special formatting (e.g., HTML, Markdown, LaTeX).\n",
    "- Do not format your output with lists using hyphens or bullet points \n",
    "- Do not use any bold text (i.e. **bold**) or formatted headers. \n",
    "- Do not add any explanations or extra text beyond the required output format.\n",
    "- Do not include any additional explanations or words in the output beyond what is outlines under 'Expected Output Format:' in these instruction. \n",
    "\n",
    "---\n",
    "\n",
    "Expected Output Format:  \n",
    "\n",
    "Technical Skills:  \n",
    "[Technical Skill Keyword], (Necessity Score: [Insert Relevance Score Here]), (Skill Level Score: [Insert Skill Level Label Here])\n",
    "\n",
    "---\n",
    "\n",
    "'''\n",
    "\n",
    "# pos Industry of Interest, Soft Skills, and Company Values/Culture/Mission extraction instructions\n",
    "P_In_SS_V_EOB_instructions = '''\n",
    "---\n",
    "\n",
    "Task Description:  \n",
    "You are an AI designed to extract and label keywords from job position profiles for internships. Your objective is to identify and categorize keywords into three groups: Industry,  Soft Skills, and Company Values/Culture/Mission. Each keyword category has specific extraction and labeling criteria.\n",
    "\n",
    "---\n",
    "\n",
    "Input Information:  \n",
    "You will receive a job position profile that includes the following sections:  \n",
    "1. Company Name  \n",
    "2. Position Title  \n",
    "3. Position Description  \n",
    "4. Position Other Skill/Requirements/Preferences\n",
    "\n",
    "---\n",
    "\n",
    "Extraction Criteria:\n",
    "\n",
    "1. Industry:\n",
    "   - Identify keywords related to the industry of the job.\n",
    "   - If the Position Title includes information relevant to the job's industry focus, extract them as keywords.\n",
    "   - Treat the Position Description and Position Other Skill/Requirements/Preferences Summary sections as distinct entities. Extract keywords separately from each section. However, do not duplicate keywords.\n",
    "   \n",
    "2. Soft Skills:\n",
    "   - Identify keywords that represent the soft skills required or preferred for the job position.\n",
    "   - Each soft skill should be on its own line. For example, if the position profile states 'Strong communication & organizational skills', extract 'Strong communication' as one keyword and 'Organizational skills' as another.\n",
    "   - Separate skills. If 'Team player (collaboration, communication)' is in the position profile, extract 'Team player', 'Collaboration', and 'Communication' as separate keywords.\n",
    "   - Ensure ALL soft skills required or preferred for the position are extracted from the profile.\n",
    "   - Treat the Position Description and Other Skill/Requirements/Preferences Summary sections as distinct entities. Extract keywords separately from each section. However, do not duplicate keywords.\n",
    "\n",
    "3. Company Values, Culture, and Mission:\n",
    "   - Identify keywords that represent the company's values, culture, and mission.\n",
    "\n",
    "---\n",
    "\n",
    "Labeling Criteria:\n",
    "\n",
    "1. Industry:\n",
    "   - Label each industry keyword with:\n",
    "     - Primary: The main industries of the job.\n",
    "     - Secondary: Closely related industries.\n",
    "     - Tertiary: Industries less related to the main industries.\n",
    "   - If not explicitly stated, assume the industry label (Primary, Secondary, or Tertiary) based on the context of the job position profile. If it's still unclear, default to Secondary.\n",
    "   \n",
    "2. Soft Skills:\n",
    "   - Each soft skill keyword should be labeled with a \"Skill Level Score.\"\n",
    "   - Skill Level Score:\n",
    "      - The Skill Level Score measures the level of proficiency or expertise required for each soft skill in the job position.\n",
    "      - To determine the score, consider the full context of the job position profile, including the Position Description and Position Skill/Requirements/Preferences Summary. For example:\n",
    "         - Beginner: This score is assigned to soft skills where basic knowledge or limited experience is required. The position may be entry-level or involve tasks that don't require advanced proficiency.\n",
    "         - Intermediate: This score is assigned to soft skills where a moderate level of proficiency is required. The position may require a working knowledge of the skill, with some experience applying it in various contexts.\n",
    "         - **Advanced**: This score is assigned to soft skills where a high level of proficiency is required. The position may involve complex tasks, leadership roles, or situations where extensive experience and expertise in the skill are necessary.\n",
    "   - If the required proficiency level of a skill is not explicitly stated in the profile, infer the appropriate score based on the overall context provided by the job description and requirements. If the level of proficiency is still unclear, default to labeling it as Intermediate.\n",
    "\n",
    "3. Company Values, Culture, and Mission:\n",
    "   - No labeling is required for these keywords; simply extract the relevant terms.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Output Expectations:\n",
    "\n",
    "- It is CRITICAL that you adhere strictly to the format provided below.\n",
    "- Output should be in plain text without any special formatting (e.g., HTML, Markdown, LaTeX).\n",
    "- Do not format your output with lists using hyphens or bullet points.\n",
    "- Do not use any bold text (i.e. **bold**) or formatted headers.\n",
    "- Do not add any explanations or extra text beyond the required output format.\n",
    "- Do not include any additional explanations or words in the output beyond what is outlined under 'Expected Output Format:' in these instructions.\n",
    "\n",
    "---\n",
    "\n",
    "Expected Output Format:\n",
    "\n",
    "Industries:  \n",
    "[Industry Keyword], (Industry Label: [Insert Industry Label Here])\n",
    "\n",
    "Soft Skills:  \n",
    "[Soft Skill Keyword], (Skill Level Score: [Insert Skill Level Label Here])\n",
    "\n",
    "Company Values, Culture, and Mission:  \n",
    "[Company Values, Culture, and Mission Keyword]\n",
    "\n",
    "---\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grouped Keyword Extraction Code: All Qualitative Data</h3>\n",
    "\n",
    "<p>This code is part of the Extract-O-Bot pipeline stage, with the primary objective of extracting keywords from student applications and job postings based on specific instructions. The extraction process is tailored to identify and categorize keywords related to technical skills, education, industry of interest, soft skills, and company values/culture/mission.</p>\n",
    "\n",
    "<h4>Key Extraction Instructions:</h4>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Student Technical Skills and Education (S_TS_Ed):</strong> Extracts and labels technical skill and educational background keywords from student profiles. The extraction is based on the resume, essays, and education summary provided by the applicant. Technical skills are labeled with a Relevance Score and a Skill Level Score, while educational keywords are simply extracted without labels.</li>\n",
    "    <li><strong>Student Industry of Interest, Soft Skills, and Values/Motivations/Mission (S_In_SS_V):</strong> Extracts and labels keywords related to the applicant's industry of interest, soft skills, and values/motivations/mission. Keywords are inferred from the student's entire profile, and specific labeling criteria are applied to each category.</li>\n",
    "    <li><strong>Position Technical Skills (P_TS):</strong> Extracts and labels technical skill keywords from job position profiles. The process considers various sections of the job description, including the position title, description, and any additional requirements or preferences.</li>\n",
    "    <li><strong>Position Industry of Interest, Soft Skills, and Company Values/Culture/Mission (P_In_SS_V):</strong> Extracts and labels keywords related to the industry of the job, required soft skills, and the company's values, culture, and mission. Each keyword category follows specific extraction and labeling criteria.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Key Functions:</h4>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>load_and_combine_saved_dfs:</strong> Loads and combines multiple saved DataFrames from a specified directory, removing duplicates. This function is crucial for resuming processing from where it left off in case of timeouts or interruptions.</li>\n",
    "    <li><strong>restart_processing:</strong> Restarts the keyword extraction process by loading previously saved progress and updating the DataFrame to mark already processed profiles. It ensures that the process continues smoothly without reprocessing already completed tasks.</li>\n",
    "    <li><strong>save_temp_dataframe:</strong> Saves a temporary DataFrame containing only the processed rows to disk, used primarily in case of timeouts to ensure progress isn't lost.</li>\n",
    "    <li><strong>process_keywords:</strong> Organizes and categorizes the extracted keywords into predefined categories, handling special cases for specific keyword formatting.</li>\n",
    "    <li><strong>replenish_rps & replenish_tps:</strong> Continuously replenishes the rate and token limit semaphores to allow more requests and tokens per second, ensuring the API limits are respected while processing the data.</li>\n",
    "    <li><strong>extract_o_bot:</strong> The core function that handles the interaction with the OpenAI API, sending formatted profiles for keyword extraction, managing rate limits, and handling errors like rate limit exceeded or generic failures.</li>\n",
    "    <li><strong>clean_text:</strong> Prepares and formats the text data for a given profile, ensuring it is ready for keyword extraction. The formatting is customized based on whether the profile is a student or a position.</li>\n",
    "    <li><strong>extract_keywords_single_profile:</strong> Extracts keywords from a single profile by first cleaning the text and then processing the keywords. The function handles different return cases based on the profile type and keyword category.</li>\n",
    "    <li><strong>process_batch & process_batch_with_timeout:</strong> Processes batches of profiles for keyword extraction using concurrent processing. The functions manage batch sizes based on token limits and RPM constraints, incorporating a timeout mechanism to handle potential long-running operations.</li>\n",
    "    <li><strong>get_batch:</strong> Manages the extraction of keywords for profiles in the DataFrame by processing them in batches. It calculates the batch size based on token and RPM constraints and handles the processing efficiently.</li>\n",
    "    <li><strong>execute_keyword_extractions:</strong> Coordinates the keyword extraction process across multiple types of profiles and keyword categories, iterating through predefined extraction jobs.</li>\n",
    "    <li><strong>estimate_tokens:</strong> Estimates the number of tokens required to process a given text using a specified model, aiding in planning and resource allocation.</li>\n",
    "    <li><strong>calculate_profile_tokens:</strong> Calculates the estimated token counts for different types of profile extractions, assigning token counts based on the profile type and model used.</li>\n",
    "    <li><strong>estimate_processing_time:</strong> Estimates the total processing time required based on the number of tokens and the API rate limits, helping to manage expectations and scheduling.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Additional Details:</h4>\n",
    "<ul>\n",
    "    <li><strong>Model Selection:</strong> The script is set to use the 'gpt-4o-mini' model by default for testing, as it's 50x cheaper than 'gpt-4o'. For final runs, 'gpt-4o' can be used for better output quality.</li>\n",
    "    <li><strong>API Rate Limits:</strong> Despite being on a higher API tier, the rate limiting experience has led to setting a safer threshold of 150 requests per minute, balancing efficiency and stability.</li>\n",
    "    <li><strong>Token and Price Estimation:</strong> The script includes detailed calculations for token usage and cost estimation based on the selected model, ensuring users are aware of the processing requirements and associated costs.</li>\n",
    "    <li><strong>Final Save and Data Combination:</strong> After processing, the final DataFrame is saved and combined with any previously saved output, ensuring that the results are consolidated and ready for further analysis or use.</li>\n",
    "</ul>\n",
    "\n",
    "<p>This code automates the extraction of relevant keywords from student and job profiles, facilitating a more efficient and scalable approach to matching candidates with opportunities. The extensive use of threading, error handling, and resource management ensures that the process is robust and can handle large datasets within API constraints.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped keyword extraction code. All qualitative data.\n",
    "\n",
    "def load_and_combine_saved_dfs(temp_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Function Name: load_and_combine_saved_dfs\n",
    "    Purpose/Description:\n",
    "        Loads and combines multiple saved DataFrames from the specified directory.\n",
    "        The DataFrames are combined and duplicates are removed.\n",
    "    Parameters:\n",
    "        - temp_dir (str): The directory where the saved DataFrames are located.\n",
    "        - base_filename (str): The base filename pattern to match when loading files.\n",
    "    Return Value:\n",
    "        - pandas.DataFrame: A combined DataFrame containing all the loaded data, or an empty DataFrame if no files are found.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all files matching the pattern in the specified directory\n",
    "    all_files = glob.glob(os.path.join(temp_dir, f\"{base_filename}_*.parquet\"))\n",
    "    \n",
    "    # Load all found DataFrames into a list\n",
    "    df_list = [pd.read_parquet(file) for file in all_files]\n",
    "    \n",
    "    # If any DataFrames were loaded, combine them and remove duplicates\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True).drop_duplicates()\n",
    "        print(f\"Combined {len(df_list)} saved DataFrames.\")\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()  # If no files were found, return an empty DataFrame\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def restart_processing(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, max_rpm, profile_type, keyword_cats, assistant_id):\n",
    "    \"\"\"\n",
    "    Function Name: restart_processing\n",
    "    Purpose/Description:\n",
    "        Restarts the keyword extraction process by loading previously saved progress and resuming from where it left off.\n",
    "        Updates the DataFrame to mark already processed profiles and continues processing the remaining ones.\n",
    "    Parameters:\n",
    "        - dataframe (pandas.DataFrame): The DataFrame containing profiles to process.\n",
    "        - pbar (tqdm.tqdm): The progress bar object to update progress.\n",
    "        - timeout_dir (str): The directory where temporary saved files are located.\n",
    "        - periodic_save_dir (str): The directory to save periodic checkpoint files.\n",
    "        - base_filename (str): The base name for saving and loading checkpoint files.\n",
    "        - max_tokens_per_minute (int): The maximum number of tokens that can be processed per minute.\n",
    "        - max_rpm (int): The maximum number of requests that can be processed per minute.\n",
    "        - profile_type (str): The type of profile ('stu' for student or 'pos' for position).\n",
    "        - keyword_cats (str): The category of keywords to extract.\n",
    "        - assistant_id (str): The ID of the assistant responsible for keyword extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and combine previously saved DataFrames\n",
    "    combined_df = load_and_combine_saved_dfs(timeout_dir, base_filename)\n",
    "    \n",
    "    # Determine the correct ID column based on the profile type\n",
    "    id_column = f'{profile_type}_(Do Not Modify) Job Posting' if profile_type == 'pos' else f'{profile_type}_(Do Not Modify) Application'\n",
    "    \n",
    "    # Drop duplicates to ensure unique profiles\n",
    "    combined_unique_profiles = combined_df.drop_duplicates(subset=[id_column])\n",
    "    \n",
    "    # Define the processed flag column for the keyword category\n",
    "    processed_flag = f\"{keyword_cats}_processed\"\n",
    "    \n",
    "    # Mark profiles as processed if they exist in the combined DataFrame\n",
    "    if not combined_df.empty:\n",
    "        dataframe.loc[dataframe[id_column].isin(combined_df[id_column]), processed_flag] = True\n",
    "    \n",
    "    # Identify remaining profiles that need to be processed\n",
    "    remaining_df = dataframe[~dataframe[processed_flag]]\n",
    "    \n",
    "    # Drop duplicates from the remaining profiles\n",
    "    remaining_unique_profiles = remaining_df.drop_duplicates(subset=[id_column])\n",
    "\n",
    "    # Check if there are any profiles left to process\n",
    "    if remaining_df.empty:\n",
    "        print(\"No remaining rows to process. All data has been processed.\")\n",
    "    else:\n",
    "        print(f\"{len(combined_unique_profiles)} profiles have been processed for {keyword_cats} extraction. {len(remaining_unique_profiles)} profiles remaining.\")\n",
    "        # Continue processing the remaining profiles\n",
    "        get_batch(remaining_df, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, max_rpm, profile_type, keyword_cats, assistant_id)\n",
    "\n",
    "\n",
    "def save_temp_dataframe(dataframe, timeout_dir, base_filename, keyword_cats):\n",
    "    \"\"\"\n",
    "    Function Name: save_temp_dataframe\n",
    "    Purpose/Description:\n",
    "        Saves a temporary DataFrame containing only the processed rows to disk. Used in case of timeouts to save progress.\n",
    "    Parameters:\n",
    "        - dataframe (pandas.DataFrame): The DataFrame containing profiles to save.\n",
    "        - timeout_dir (str): The directory where the temporary files will be saved.\n",
    "        - base_filename (str): The base name for the saved file.\n",
    "        - keyword_cats (str): The category of keywords that have been processed.\n",
    "    Return Value:\n",
    "        - str: The path to the saved file, or None if no rows were saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the DataFrame to include only processed rows\n",
    "    processed_dataframe = dataframe[dataframe[f\"{keyword_cats}_processed\"] == True]\n",
    "    \n",
    "    if not processed_dataframe.empty:\n",
    "        # Generate a timestamp for the filename\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Create the full path for the temporary file\n",
    "        temp_path = os.path.join(timeout_dir, f\"{base_filename}_{timestamp}.parquet\")\n",
    "        # Save the processed DataFrame to the temporary file\n",
    "        processed_dataframe.to_parquet(temp_path)\n",
    "        print(f\"Saved processed rows to {temp_path} due to timeout.\")\n",
    "        return temp_path\n",
    "    else:\n",
    "        print(\"No rows to save; skipping save.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_keywords(keywords, keyword_cat):\n",
    "    \"\"\"\n",
    "    Function Name: process_keywords\n",
    "    Purpose/Description:\n",
    "        Processes extracted keywords, organizing them into predefined categories.\n",
    "        Handles special cases for specific keyword formatting.\n",
    "    Parameters:\n",
    "        - keywords (str): The raw keyword string extracted from the profile.\n",
    "        - keyword_cat (str): The category of keywords being processed (e.g., 'S_TS_Ed', 'P_TS').\n",
    "    Return Value:\n",
    "        - tuple: Processed keywords for each category based on the input keyword_cat.\n",
    "    \"\"\"\n",
    "    \n",
    "    #region Initial processing for special cases. Add more as needed.\n",
    "    keywords = keywords.replace('Natural Language Processing (NLP)', 'Natural Language Processing')\n",
    "    keywords = keywords.replace('(NLP)', 'Natural Language Processing')\n",
    "    #endregion\n",
    "    \n",
    "    # Dictionary to store the processed keywords for each category\n",
    "    processed_keywords = {\n",
    "        'Technical Skills:': \"\",\n",
    "        'Education:': \"\",\n",
    "        'Industries of Interest:': \"\",\n",
    "        'Soft Skills:': \"\",\n",
    "        'Values, Motivations, and Mission:': \"\",\n",
    "        'Industries:': \"\",\n",
    "        'Company Values, Culture, and Mission:': \"\"\n",
    "    }\n",
    "\n",
    "    # Split the input keywords into lines\n",
    "    lines = keywords.splitlines()\n",
    "    # Remove empty lines and strip whitespace\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    current_section = None\n",
    "\n",
    "    # Iterate through the lines to categorize them\n",
    "    for line in lines:\n",
    "        line = line.strip()  # Trim whitespace from the start and end\n",
    "        if line in processed_keywords:\n",
    "            current_section = line  # Set the current section based on the header\n",
    "        elif current_section:\n",
    "            # Add the line to the appropriate section\n",
    "            processed_keywords[current_section] += line + \"\\n\"\n",
    "    \n",
    "    # Return results based on the keyword category\n",
    "    if keyword_cat == 'S_TS_Ed':\n",
    "        return (\n",
    "            processed_keywords['Technical Skills:'].strip(),\n",
    "            processed_keywords['Education:'].strip()\n",
    "        )\n",
    "    elif keyword_cat == 'S_In_SS_V':\n",
    "        return (\n",
    "            processed_keywords['Industries of Interest:'].strip(),\n",
    "            processed_keywords['Soft Skills:'].strip(),\n",
    "            processed_keywords['Values, Motivations, and Mission:'].strip()\n",
    "        )\n",
    "    elif keyword_cat == 'P_TS':\n",
    "        return processed_keywords['Technical Skills:'].strip()\n",
    "    elif keyword_cat == 'P_In_SS_V':\n",
    "        return (\n",
    "            processed_keywords['Industries:'].strip(),\n",
    "            processed_keywords['Soft Skills:'].strip(),\n",
    "            processed_keywords['Company Values, Culture, and Mission:'].strip()\n",
    "        )\n",
    "\n",
    "\n",
    "def replenish_rps():\n",
    "    \"\"\"\n",
    "    Function Name: replenish_rps\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Continuously replenishes the rate limit semaphore to allow more requests per second (RPS). \n",
    "    This function runs in a loop, waiting for the signal that requests have started and \n",
    "    then replenishing the semaphore periodically.\n",
    "    These values are set in the 'Threads for resource management' region.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        time.sleep(0.1)  # Short delay to reduce CPU usage\n",
    "        requests_started.wait()  # Wait until the request process starts\n",
    "        while True:\n",
    "            time.sleep(5)  # Wait before replenishing the rate limit\n",
    "            for _ in range(rps - rate_limit._value):\n",
    "                rate_limit.release()  # Release the semaphore to allow more requests\n",
    "\n",
    "\n",
    "def replenish_tps():\n",
    "    \"\"\"\n",
    "    Function Name: replenish_tps\n",
    "    \n",
    "    Purpose/Description:\n",
    "    Continuously replenishes the token limit semaphore to allow more tokens per second (TPS) \n",
    "    to be processed. This function runs in a loop, waiting for the signal that requests \n",
    "    have started and then replenishing the semaphore periodically. \n",
    "    These values are set in the 'Threads for resource management' region. \n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Return Value:\n",
    "    None\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        time.sleep(0.1)  # Short delay to reduce CPU usage\n",
    "        requests_started.wait()  # Wait until the request process starts\n",
    "        while True:\n",
    "            time.sleep(1)  # Wait before replenishing the token limit\n",
    "            tokens_to_add = tps - token_limit._value  # Calculate how many tokens to add\n",
    "            if tokens_to_add > 0:\n",
    "                token_limit.release(tokens_to_add)  # Replenish the semaphore with the required tokens\n",
    "\n",
    "\n",
    "def extract_o_bot(formatted_profile, assistant_id, row, keyword_cats):\n",
    "    global total_tokens_used, total_input_tokens_used, total_output_tokens_used, threads_in_loop\n",
    "    try:\n",
    "        \n",
    "        tokens_needed = row[f'TokenCount_{keyword_cats}']\n",
    "        token_limit.acquire(tokens_needed)\n",
    "        rate_limit.acquire()\n",
    "        \n",
    "        if not requests_started.is_set():\n",
    "            requests_started.set()  # Signal that requests have started\n",
    "            \n",
    "        while True:\n",
    "            try:\n",
    "                # Create a new thread and send the formatted profile as a message\n",
    "                thread = client.beta.threads.create()\n",
    "                message = client.beta.threads.messages.create(thread_id=thread.id, content=formatted_profile, role=\"user\")\n",
    "                run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=assistant_id)\n",
    "                break  # Exit loop if successful\n",
    "            except Exception as e:\n",
    "                # Handle rate limit exceeded error (HTTP 429)\n",
    "                if '429' in str(e):\n",
    "                    print(\"Rate limit exceeded. Retrying after a delay.\")\n",
    "                    print(\"Error:\", e)\n",
    "                    time.sleep(5)  # Wait before retrying\n",
    "                    continue\n",
    "                # Handle bad request error (HTTP 400)\n",
    "                elif '400' in str(e):\n",
    "                    print(e)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e  # Raise other exceptions\n",
    "                \n",
    "        with threads_lock:\n",
    "            threads_in_loop += 1  # Increment active threads count\n",
    "        \n",
    "        while True:\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                # Check the status of the run\n",
    "                run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "                if run_status.status == \"completed\":\n",
    "                        \n",
    "                    # Update the total tokens used globally\n",
    "                    total_input_tokens_used_in_run = run_status.usage.prompt_tokens\n",
    "                    total_input_tokens_used += total_input_tokens_used_in_run\n",
    "                    \n",
    "                    total_output_tokens_used_in_run = run_status.usage.completion_tokens\n",
    "                    total_output_tokens_used += total_output_tokens_used_in_run\n",
    "                    \n",
    "                    total_tokens_used_in_run = run_status.usage.total_tokens\n",
    "                    total_tokens_used += total_tokens_used_in_run\n",
    "                    \n",
    "                    break  # Exit loop when processing is complete\n",
    "\n",
    "                elif run_status.status == \"failed\":\n",
    "                    # Handle rate limit exceeded within the run\n",
    "                    if run_status.last_error.code == 'rate_limit_exceeded':\n",
    "                        match1 = re.search(r'in (\\d+\\.\\d+)s', run_status.last_error.message)\n",
    "                        match2 = re.search(r'in (\\d+)ms', run_status.last_error.message)\n",
    "                        if match1:\n",
    "                            wait_time = float(match1.group(1)) + 1\n",
    "                            print(run_status.last_error.message)\n",
    "                            print('#ERROR3#: Rate limit exceeded. Waiting for', wait_time, 'seconds.')\n",
    "                            with threads_lock:\n",
    "                                threads_in_loop -= 1\n",
    "                            time.sleep(wait_time)\n",
    "                            return extract_o_bot(formatted_profile, assistant_id, row, keyword_cats)\n",
    "                        elif match2:\n",
    "                            wait_time = float(match2.group(1)) / 1000.0 + 1  # Convert milliseconds to seconds\n",
    "                            print(run_status.last_error.message)\n",
    "                            print('#ERROR3#: Rate limit exceeded. Waiting for', wait_time, 'seconds.')\n",
    "                            with threads_lock:\n",
    "                                threads_in_loop -= 1\n",
    "                            time.sleep(wait_time)\n",
    "                            return extract_o_bot(formatted_profile, assistant_id, row, keyword_cats)\n",
    "                    # Handle a generic error message\n",
    "                    elif 'Sorry, something went wrong' in str(run_status.last_error):\n",
    "                        print(\"Something went wrong. Trying again\")\n",
    "                        time.sleep(2)\n",
    "                        with threads_lock:\n",
    "                            threads_in_loop -= 1\n",
    "                        return extract_o_bot(formatted_profile, assistant_id, row, keyword_cats)\n",
    "                    else:\n",
    "                        print(\"Run failed. Trying again\", run_status.last_error)\n",
    "                        with threads_lock:\n",
    "                            threads_in_loop -= 1\n",
    "                        return extract_o_bot(formatted_profile, assistant_id, row, keyword_cats)   \n",
    "            except RateLimitError as e:\n",
    "                print('#ERROR2#: Rate limit exceeded.')\n",
    "                time.sleep(2)\n",
    "                with threads_lock:\n",
    "                    threads_in_loop -= 1\n",
    "                return extract_o_bot(formatted_profile, assistant_id, row, keyword_cats)   \n",
    "            except Exception as e:\n",
    "                if 'rate_limit_exceeded' in str(e):\n",
    "                    print('#ERROR2#: Rate limit exceeded. waiting and retrying.')\n",
    "                    with threads_lock:\n",
    "                        threads_in_loop -= 1\n",
    "                    time.sleep(10)\n",
    "                    return extract_o_bot(formatted_profile, assistant_id, row, keyword_cats)\n",
    "                else:\n",
    "                    print(\"Error:\", e)\n",
    "                    raise e  # Raise other exceptions\n",
    "                \n",
    "        with threads_lock:\n",
    "            threads_in_loop -= 1  # Decrement active threads count\n",
    "\n",
    "        # Retrieve and return the result message from the assistant\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        for message in reversed(messages.data):\n",
    "            role = message.role  \n",
    "            for content in message.content:\n",
    "                if content.type == 'text' and role == 'assistant':\n",
    "                    return content.text.value, total_tokens_used_in_run, total_input_tokens_used_in_run, total_output_tokens_used_in_run\n",
    "\n",
    "    except RateLimitError as e:\n",
    "        print('#ERROR1#: Rate limit exceeded.')\n",
    "        time.sleep(3)\n",
    "    except Exception as e:\n",
    "        if 'rate_limit_exceeded' in str(e):\n",
    "            time.sleep(3)\n",
    "            return extract_o_bot(formatted_profile, assistant_id, row, keyword_cats)\n",
    "        else:\n",
    "            raise e  # Raise other exceptions if not related to rate limit\n",
    "\n",
    "\n",
    "def clean_text(row, profile_type):\n",
    "    \"\"\"\n",
    "    Function Name: clean_text\n",
    "    Purpose/Description:\n",
    "        Cleans and formats the text data for a given profile, preparing it for keyword extraction.\n",
    "        The formatting is based on the profile type (student or position).\n",
    "    Parameters:\n",
    "        - row (pandas.Series): A row from the DataFrame containing profile data.\n",
    "        - profile_type (str): The type of profile ('stu' for student or 'pos' for position).\n",
    "    Return Value:\n",
    "        - str: A formatted string containing the relevant information from the profile.\n",
    "    \"\"\"\n",
    "\n",
    "    if profile_type == 'stu':\n",
    "        # Format the text for a student profile\n",
    "        formatted_text = f\"\"\"\n",
    "        ----------------------------------------\n",
    "        Applicant Profile:\n",
    "        ----------------------------------------\n",
    "\n",
    "        Applicant Name: {row['stu_Legal Name']}\n",
    "\n",
    "        Applicant Resume: \n",
    "        ----------------------------------------\n",
    "        {row['stu_Resume_text']}\n",
    "\n",
    "        Applicant Essays:\n",
    "        ----------------------------------------\n",
    "\n",
    "        Essay - Dream Companies\n",
    "        {row['stu_Essay - Dream Companies']}\n",
    "\n",
    "        Essay - Experience in field of Study\n",
    "        {row['stu_Essay - Experience in field of Study']}\n",
    "\n",
    "        Essay - Influence on interest in the field\n",
    "        {row['stu_Essay - Influence on interest in the field']}\n",
    "\n",
    "        Essay - Internship Career Goals\n",
    "        {row['stu_Essay - Internship Career Goals']}\n",
    "\n",
    "        Essay - Overall Career Goals\n",
    "        {row['stu_Essay - Overall Career Goals']}\n",
    "\n",
    "        Applicant Skill/Education Summary\n",
    "        ----------------------------------------\n",
    "\n",
    "        {row['stu_position education summary']}\n",
    "        \"\"\"\n",
    "        return formatted_text\n",
    "\n",
    "    elif profile_type == 'pos':\n",
    "        # Format the text for a position profile\n",
    "        formatted_text = f\"\"\" \n",
    "        -----------------------------------------------\n",
    "        Position Profile \n",
    "        -----------------------------------------------\n",
    "\n",
    "        Company Name: {row['pos_Company']}\n",
    "        Position Title: {row['pos_Name']}\n",
    "\n",
    "        Position Description:\n",
    "        -----------------------------------------------\n",
    "\n",
    "        {row['pos_Job_desc_text']}\n",
    "\n",
    "        Other Skills/Requirements/Preferences Summary\n",
    "        -----------------------------------------------\n",
    "\n",
    "        Other requirements/preferences: \n",
    "        {row['pos_Other requirements/preferences']}\n",
    "\n",
    "        Other Skills:\n",
    "        {row['pos_Other Skills']}\n",
    "\n",
    "        Position Summary:\n",
    "        {row['pos_External Position Summary']}\n",
    "        \"\"\"\n",
    "        return formatted_text\n",
    "\n",
    "\n",
    "def extract_keywords_single_profile(row, profile_type, keyword_cats, assistant_id, id_column):\n",
    "    \"\"\"\n",
    "    Function Name: extract_keywords_single_profile\n",
    "    Purpose/Description:\n",
    "        Extracts keywords from a single profile by first cleaning the text and then processing the keywords.\n",
    "    Parameters:\n",
    "        - row (pandas.Series): A row from the DataFrame containing profile data.\n",
    "        - profile_type (str): The type of profile ('stu' for student or 'pos' for position).\n",
    "        - keyword_cats (str): The category of keywords to extract (e.g., 'S_TS_Ed', 'P_TS').\n",
    "        - assistant_id (str): The ID of the assistant responsible for keyword extraction.\n",
    "        - id_column (str): The name of the column containing the unique ID for the profile.\n",
    "    Return Value:\n",
    "        - tuple: A tuple containing the profile ID, extracted keywords, and token usage statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text for the profile\n",
    "    cleaned_text = clean_text(row, profile_type)\n",
    "\n",
    "    # Extract keywords from the cleaned text using the specified assistant\n",
    "    keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used = extract_o_bot(cleaned_text, assistant_id, row, keyword_cats)\n",
    "\n",
    "    # Process the extracted keywords to categorize them appropriately\n",
    "    processed_keywords = process_keywords(keywords, keyword_cats)\n",
    "\n",
    "    # Handle the different return cases based on the profile type and keyword category\n",
    "    if keyword_cats == 'S_TS_Ed':\n",
    "        # Handle technical skills and education for student profiles\n",
    "        tech_keywords, edu_keywords = processed_keywords\n",
    "        return row[id_column], tech_keywords, edu_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used\n",
    "    \n",
    "    elif keyword_cats == 'S_In_SS_V':\n",
    "        # Handle industry, soft skills, and values for student profiles\n",
    "        industry_keywords, soft_keywords, values_keywords = processed_keywords\n",
    "        return row[id_column], industry_keywords, soft_keywords, values_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used\n",
    "    \n",
    "    elif keyword_cats == 'P_TS':\n",
    "        # Handle technical skills for position profiles\n",
    "        tech_keywords = processed_keywords\n",
    "        return row[id_column], tech_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used\n",
    "    \n",
    "    elif keyword_cats == 'P_In_SS_V':\n",
    "        # Handle industry, soft skills, and values for position profiles\n",
    "        industry_keywords, soft_keywords, values_keywords = processed_keywords\n",
    "        return row[id_column], industry_keywords, soft_keywords, values_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used\n",
    "\n",
    "\n",
    "def process_batch(dataframe, pbar, periodic_save_dir, base_filename, profile_type, keyword_cats, assistant_id, batch, num_in_batch, batch_completed):\n",
    "    \"\"\"\n",
    "    Function Name: process_batch\n",
    "    Purpose/Description:\n",
    "        Processes a batch of profiles for keyword extraction using concurrent processing. Updates the DataFrame with extracted keywords.\n",
    "    Parameters:\n",
    "        - dataframe (pandas.DataFrame): The DataFrame containing profiles to process.\n",
    "        - pbar (tqdm.tqdm): The progress bar object to update progress.\n",
    "        - periodic_save_dir (str): The directory to save periodic checkpoint files.\n",
    "        - base_filename (str): The base name for saving checkpoint files.\n",
    "        - profile_type (str): The type of profile ('stu' for student or 'pos' for position).\n",
    "        - keyword_cats (str): The category of keywords to extract.\n",
    "        - assistant_id (str): The ID of the assistant responsible for keyword extraction.\n",
    "        - batch (pandas.DataFrame): The batch of profiles to process in this iteration.\n",
    "        - num_in_batch (int): The number of profiles in the current batch.\n",
    "        - batch_completed (threading.Event): Event to signal that the batch has been processed.\n",
    "    \"\"\"\n",
    "\n",
    "    global total_keyword_extractions, num_keyword_extractions\n",
    "    id_column = f'{profile_type}_(Do Not Modify) Job Posting' if profile_type == 'pos' else f'{profile_type}_(Do Not Modify) Application'\n",
    "    workers = num_in_batch  # Number of worker threads to use, equal to the number of profiles in the batch\n",
    "\n",
    "    # Use a ThreadPoolExecutor to process profiles concurrently\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit the keyword extraction task for each profile in the batch\n",
    "        future_to_profile = {\n",
    "            executor.submit(extract_keywords_single_profile, row, profile_type, keyword_cats, assistant_id, id_column)\n",
    "            for idx, row in batch.iterrows()\n",
    "        }\n",
    "\n",
    "        # Process the results as they are completed\n",
    "        for future in as_completed(future_to_profile):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                # Handle different return types based on keyword category\n",
    "                if keyword_cats == 'S_TS_Ed':\n",
    "                    profile_id, tech_keywords, edu_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used = result\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_technical_keywords'] = tech_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_education_keywords'] = edu_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Tokens_Used_{keyword_cats}'] = actual_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Input_Tokens_Used_{keyword_cats}'] = actual_input_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Output_Tokens_Used_{keyword_cats}'] = actual_output_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, 'S_TS_Ed_processed'] = True\n",
    "                \n",
    "                elif keyword_cats == 'S_In_SS_V':\n",
    "                    profile_id, industry_keywords, soft_keywords, values_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used = result\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_industry_keywords'] = industry_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_soft_keywords'] = soft_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_values_keywords'] = values_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Tokens_Used_{keyword_cats}'] = actual_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Input_Tokens_Used_{keyword_cats}'] = actual_input_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Output_Tokens_Used_{keyword_cats}'] = actual_output_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, 'S_In_SS_V_processed'] = True\n",
    "                \n",
    "                elif keyword_cats == 'P_TS':\n",
    "                    profile_id, tech_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used = result\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_technical_keywords'] = tech_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Tokens_Used_{keyword_cats}'] = actual_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Input_Tokens_Used_{keyword_cats}'] = actual_input_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Output_Tokens_Used_{keyword_cats}'] = actual_output_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, 'P_TS_processed'] = True\n",
    "                \n",
    "                elif keyword_cats == 'P_In_SS_V':\n",
    "                    profile_id, industry_keywords, soft_keywords, values_keywords, actual_tokens_used, actual_input_tokens_used, actual_output_tokens_used = result\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_industry_keywords'] = industry_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_soft_keywords'] = soft_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'{profile_type}_values_keywords'] = values_keywords\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Tokens_Used_{keyword_cats}'] = actual_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Input_Tokens_Used_{keyword_cats}'] = actual_input_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, f'Actual_Output_Tokens_Used_{keyword_cats}'] = actual_output_tokens_used\n",
    "                    dataframe.loc[dataframe[id_column] == profile_id, 'P_In_SS_V_processed'] = True\n",
    "\n",
    "                # Update the global keyword extraction count and save progress periodically\n",
    "                num_keyword_extractions += 1\n",
    "                if num_keyword_extractions % 50 == 0:\n",
    "                    dataframe.to_parquet(f\"{periodic_save_dir}/{base_filename}_{num_keyword_extractions}.parquet\")\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "            except Exception as e:\n",
    "                print(\"Error processing profile:\", e)\n",
    "                continue\n",
    "        \n",
    "        batch_completed.set()  # Signal that the batch has been completed\n",
    "         \n",
    "    \n",
    "def process_batch_with_timeout(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, profile_type, keyword_cats, assistant_id, batch, num_in_batch):\n",
    "    \"\"\"\n",
    "    Function Name: process_batch_with_timeout\n",
    "    Purpose/Description:\n",
    "        Processes a batch of profiles with a timeout mechanism to handle potential long-running operations.\n",
    "        It triggers a countdown timer and a timeout handler to save progress if processing takes too long.\n",
    "    Parameters:\n",
    "        - dataframe (pandas.DataFrame): The DataFrame containing profiles to process.\n",
    "        - pbar (tqdm.tqdm): The progress bar object to update progress.\n",
    "        - timeout_dir (str): The directory to save temporary files if a timeout occurs.\n",
    "        - periodic_save_dir (str): The directory to save periodic checkpoint files.\n",
    "        - base_filename (str): The base name for saving temporary and checkpoint files.\n",
    "        - profile_type (str): The type of profile ('stu' for student or 'pos' for position).\n",
    "        - keyword_cats (str): The category of keywords to extract.\n",
    "        - assistant_id (str): The ID of the assistant responsible for keyword extraction.\n",
    "        - batch (pandas.DataFrame): The batch of profiles to process in this iteration.\n",
    "        - num_in_batch (int): The number of profiles in the current batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize threading events to track the countdown and batch completion.\n",
    "    countdown_finished = threading.Event()\n",
    "    batch_completed = threading.Event()\n",
    "\n",
    "    def countdown_timer(seconds):\n",
    "        \"\"\"\n",
    "        Countdown timer that runs in a separate thread, updating the progress bar every second.\n",
    "        When the countdown finishes, it sets the countdown_finished event.\n",
    "        This is used to trigger the next batch processing step.\n",
    "        \"\"\"\n",
    "        with tqdm(total=seconds, desc=f\"Processing {num_in_batch} rows. Time Till Next Batch:\", bar_format=\"{l_bar}{bar}| {remaining} seconds\", leave=False) as pbar:\n",
    "            for i in range(seconds):\n",
    "                time.sleep(1)  # Wait for 1 second\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "        countdown_finished.set()  # Set the event to indicate the countdown has finished\n",
    "\n",
    "    def timeout_handler():\n",
    "        \"\"\"\n",
    "        Timeout handler that checks if the countdown has finished.\n",
    "        If the countdown is finished, it saves the current state and restarts the process.\n",
    "        If the countdown is still running, it ignores the timeout.\n",
    "        \"\"\"\n",
    "        if countdown_finished.is_set():\n",
    "            save_temp_dataframe(dataframe, timeout_dir, base_filename, keyword_cats)\n",
    "            print(\"Timeout occurred, saving current state and restarting process.\")\n",
    "            restart_processing(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, max_rpm, profile_type, keyword_cats, assistant_id)\n",
    "        else:\n",
    "            print(\"Timeout occurred, but batch has already started processing. Ignoring timeout.\")\n",
    "\n",
    "    # Start the countdown timer in a separate thread with a 60-second countdown.\n",
    "    countdown_thread = threading.Thread(target=countdown_timer, args=(60,))\n",
    "    countdown_thread.start()\n",
    "\n",
    "    # Start the timeout handler with a 300-second delay.\n",
    "    timeout_thread = threading.Timer(300, timeout_handler)\n",
    "    timeout_thread.start()\n",
    "\n",
    "    try:\n",
    "        # Process the batch of profiles\n",
    "        process_batch(dataframe, pbar, periodic_save_dir, base_filename, profile_type, keyword_cats, assistant_id, batch, num_in_batch, batch_completed)\n",
    "        countdown_finished.wait()  # Wait for the countdown to finish\n",
    "        batch_completed.wait()  # Wait for the batch to complete processing\n",
    "    finally:\n",
    "        # Ensure threads are cleaned up properly after processing\n",
    "        countdown_thread.join()\n",
    "        timeout_thread.cancel()\n",
    "\n",
    "\n",
    "def get_batch(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, max_rpm, profile_type, keyword_cats, assistant_id):\n",
    "    \"\"\"\n",
    "    Function Name: get_batch\n",
    "    Purpose/Description:\n",
    "        Handles the extraction of keywords for profiles in the DataFrame by processing them in batches.\n",
    "        Manages batch size based on token limits and requests per minute (RPM) constraints.\n",
    "    Parameters:\n",
    "        - dataframe (pandas.DataFrame): The DataFrame containing profiles to process.\n",
    "        - pbar (tqdm.tqdm): The progress bar object to update progress.\n",
    "        - timeout_dir (str): The directory to save temporary files if a timeout occurs.\n",
    "        - periodic_save_dir (str): The directory to save periodic checkpoint files.\n",
    "        - base_filename (str): The base name for saving temporary and checkpoint files.\n",
    "        - max_tokens_per_minute (int): The maximum number of tokens that can be processed per minute.\n",
    "        - max_rpm (int): The maximum number of requests that can be processed per minute.\n",
    "        - profile_type (str): The type of profile ('stu' for student or 'pos' for position).\n",
    "        - keyword_cats (str): The category of keywords to extract.\n",
    "        - assistant_id (str): The ID of the assistant responsible for keyword extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the correct ID and token column based on the profile type.\n",
    "    id_column = f'{profile_type}_(Do Not Modify) Job Posting' if profile_type == 'pos' else f'{profile_type}_(Do Not Modify) Application'\n",
    "    token_column = [f'TokenCount_{keyword_cats}']\n",
    "\n",
    "    unique_profiles = dataframe.drop_duplicates(subset=[id_column])  # Remove duplicate profiles to avoid double processing.\n",
    "    batch = pd.DataFrame(columns=unique_profiles.columns)  # Initialize an empty DataFrame for batching.\n",
    "    batch_token_count = 0  # Initialize the token counter for the batch.\n",
    "    num_in_batch = 0  # Initialize the counter for the number of profiles in the batch.\n",
    "\n",
    "    for idx, row in unique_profiles.iterrows():\n",
    "        # Skip profiles that have already been processed for the current keyword category.\n",
    "        if dataframe.loc[dataframe[id_column] == row[id_column], f'{keyword_cats}_processed'].values[0]:\n",
    "            continue\n",
    "        \n",
    "        current_token_count = int(row[token_column].iloc[0])  # Get the token count for the current profile.\n",
    "\n",
    "        # If adding the current profile exceeds the token or RPM limits, process the current batch.\n",
    "        if batch_token_count + current_token_count > max_tokens_per_minute or num_in_batch == max_rpm:\n",
    "            process_batch_with_timeout(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, profile_type, keyword_cats, assistant_id, batch, num_in_batch)\n",
    "\n",
    "            # Reset the batch and token counter after processing.\n",
    "            batch = pd.DataFrame(columns=unique_profiles.columns)\n",
    "            batch_token_count = 0\n",
    "            num_in_batch = 0\n",
    "        \n",
    "        # Add the current profile to the batch.\n",
    "        if batch.empty:\n",
    "            batch = pd.DataFrame([row])\n",
    "        else:\n",
    "            batch = pd.concat([batch, pd.DataFrame([row])], ignore_index=True)\n",
    "        \n",
    "        # Update the token count and batch size.\n",
    "        batch_token_count += current_token_count\n",
    "        num_in_batch += 1\n",
    "\n",
    "    # Process any remaining profiles in the final batch.\n",
    "    if not batch.empty:\n",
    "        process_batch_with_timeout(dataframe, pbar, timeout_dir, periodic_save_dir, base_filename, profile_type, keyword_cats, assistant_id, batch, num_in_batch)\n",
    "\n",
    "\n",
    "def execute_keyword_extractions(SP, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, max_rpm):\n",
    "    \"\"\"\n",
    "    Function Name: execute_keyword_extractions\n",
    "    Purpose/Description:\n",
    "        Executes the keyword extraction process for multiple types of profiles and keyword categories.\n",
    "        It iterates through predefined extraction jobs and calls the extract_keywords function for each.\n",
    "    Parameters:\n",
    "        - SP (pandas.DataFrame): The DataFrame containing profiles to process.\n",
    "        - pbar (tqdm.tqdm): The progress bar object to update progress.\n",
    "        - timeout_dir (str): The directory to save temporary files if a timeout occurs.\n",
    "        - periodic_save_dir (str): The directory to save periodic checkpoint files.\n",
    "        - base_filename (str): The base name for saving temporary and checkpoint files.\n",
    "        - max_tokens_per_minute (int): The maximum number of tokens that can be processed per minute.\n",
    "        - max_rpm (int): The maximum number of requests that can be processed per minute.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the extraction jobs for each profile type and keyword category.\n",
    "    extraction_jobs = [\n",
    "        ('stu', 'S_TS_Ed', S_TS_Ed_EOB.id),\n",
    "        ('stu', 'S_In_SS_V', S_In_SS_V_EOB.id),\n",
    "        ('pos', 'P_TS', P_TS_EOB.id),\n",
    "        ('pos', 'P_In_SS_V', P_In_SS_V_EOB.id)\n",
    "    ]\n",
    "    \n",
    "    # Iterate through the extraction jobs and process each one.\n",
    "    for profile_type, keyword_cats, assistant_id in extraction_jobs:\n",
    "        get_batch(SP, pbar, timeout_dir, periodic_save_dir, base_filename, max_tokens_per_minute, max_rpm, profile_type, keyword_cats, assistant_id)\n",
    "\n",
    "\n",
    "def estimate_tokens(text, model):\n",
    "    \"\"\"\n",
    "    Function Name: estimate_tokens\n",
    "    Purpose/Description: \n",
    "        Estimates the number of tokens required to process a given text using a specified model.\n",
    "    Parameters:\n",
    "        - text (str): The text for which to estimate the token count.\n",
    "        - model (str): The model to be used for encoding the text (e.g., 'gpt-4o-mini').\n",
    "    Return Value:\n",
    "        - int: The number of tokens required to process the text.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)  # Get the encoding for the specified model.\n",
    "    tokens = encoding.encode(text)  # Encode the text into tokens.\n",
    "    return len(tokens)  # Return the total number of tokens.\n",
    "\n",
    "\n",
    "def calculate_profile_tokens(df, profile_type, model):\n",
    "    \"\"\"\n",
    "    Function Name: calculate_profile_tokens\n",
    "    Purpose/Description: \n",
    "        Calculates the estimated token counts for different types of profile extractions in a DataFrame.\n",
    "        It assigns token counts to various columns based on the profile type (student or position) and the model used.\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The DataFrame containing the profiles to process.\n",
    "        - profile_type (str): The type of profile ('stu' for student or 'pos' for position).\n",
    "        - model (str): The model to be used for estimating tokens (e.g., 'gpt-4o-mini').\n",
    "    Return Value:\n",
    "        - pandas.DataFrame: The updated DataFrame with calculated token counts for each profile.\n",
    "    \"\"\"\n",
    "\n",
    "    #region instruction token estimation\n",
    "    # Estimated number of tokens required for instructions for each profile type and category.\n",
    "    S_TS_Ed_EOB_instructions_token_est = 825\n",
    "    S_In_SS_V_EOB_instructions_token_est = 1200\n",
    "    P_TS_EOB_instructions_token_est = 875\n",
    "    P_In_SS_V_EOB_instructions_token_est = 925\n",
    "    #endregion\n",
    "\n",
    "    #region average token output\n",
    "    # Average number of tokens expected in the output for each profile type and category.\n",
    "    S_TS_Ed_avg_token_output = 1200\n",
    "    S_In_SS_V_avg_token_output = 400\n",
    "    P_TS_avg_token_output = 1200\n",
    "    P_In_SS_V_avg_token_output = 400\n",
    "    #endregion\n",
    "\n",
    "    # Determine the correct ID column based on the profile type.\n",
    "    id_column = f'{profile_type}_(Do Not Modify) Job Posting' if profile_type == 'pos' else f'{profile_type}_(Do Not Modify) Application'\n",
    "    unique_profiles = df.drop_duplicates(subset=[id_column])  # Remove duplicate profiles to avoid double counting.\n",
    "\n",
    "    for idx, row in unique_profiles.iterrows():\n",
    "        # Generate the formatted text and calculate the token count for each unique profile.\n",
    "        formatted_text = clean_text(row, profile_type)\n",
    "        token_count = estimate_tokens(formatted_text, model)\n",
    "        \n",
    "        if profile_type == 'stu':\n",
    "            # Calculate and assign token counts for student profiles.\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenCount_S_TS_Ed'] = token_count + S_TS_Ed_EOB_instructions_token_est + S_TS_Ed_avg_token_output\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenCount_S_In_SS_V'] = token_count + S_In_SS_V_EOB_instructions_token_est + S_In_SS_V_avg_token_output\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenInputCount_S_TS_Ed'] = token_count + S_TS_Ed_EOB_instructions_token_est\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenInputCount_S_In_SS_V'] = token_count + S_In_SS_V_EOB_instructions_token_est\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenOutputCount_S_TS_Ed'] = S_TS_Ed_avg_token_output\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenOutputCount_S_In_SS_V'] = S_In_SS_V_avg_token_output\n",
    "        elif profile_type == 'pos':\n",
    "            # Calculate and assign token counts for position profiles.\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenCount_P_TS'] = token_count + P_TS_EOB_instructions_token_est + P_TS_avg_token_output\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenCount_P_In_SS_V'] = token_count + P_In_SS_V_EOB_instructions_token_est + P_In_SS_V_avg_token_output\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenInputCount_P_TS'] = token_count + P_TS_EOB_instructions_token_est\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenInputCount_P_In_SS_V'] = token_count + P_In_SS_V_EOB_instructions_token_est\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenOutputCount_P_TS'] = P_TS_avg_token_output\n",
    "            df.loc[df[id_column] == row[id_column], 'TokenOutputCount_P_In_SS_V'] = P_In_SS_V_avg_token_output\n",
    "\n",
    "    return df  # Return the DataFrame with updated token counts.\n",
    "\n",
    "\n",
    "def estimate_processing_time(df, est_total_tokens, max_tokens_per_minute, max_rpm):\n",
    "    \"\"\"\n",
    "    Function Name: estimate_processing_time\n",
    "    Purpose/Description: \n",
    "        Estimates the total processing time required based on the number of tokens and the maximum allowed tokens per minute.\n",
    "        Takes into account the rate limits of the API to determine whether tokens or requests per minute are the limiting factor.\n",
    "    Parameters:\n",
    "        - df (pandas.DataFrame): The DataFrame containing the profiles to process.\n",
    "        - est_total_tokens (int): The estimated total number of tokens required for processing.\n",
    "        - max_tokens_per_minute (int): The maximum number of tokens that can be processed per minute.\n",
    "        - max_rpm (int): The maximum number of requests that can be processed per minute.\n",
    "    Return Value:\n",
    "        - tuple: A tuple containing the estimated hours and minutes required for processing.\n",
    "    \"\"\"\n",
    "    pos_id_column = 'pos_(Do Not Modify) Job Posting'\n",
    "    stu_id_column = 'stu_(Do Not Modify) Application'\n",
    "    \n",
    "    # Count unique profiles for positions and students.\n",
    "    unique_pos_profiles = len(df.drop_duplicates(subset=[pos_id_column]))\n",
    "    unique_stu_profiles = len(df.drop_duplicates(subset=[stu_id_column]))\n",
    "    \n",
    "    # Calculate the total number of profiles to process.\n",
    "    total_profiles = unique_pos_profiles + unique_stu_profiles\n",
    "    \n",
    "    # Total number of profile processing operations (since each profile is processed twice).\n",
    "    total_processing_ops = total_profiles * 2\n",
    "    \n",
    "    # Calculate the average tokens per profile (this is an approximation).\n",
    "    avg_tokens_per_profile = est_total_tokens / total_processing_ops\n",
    "    \n",
    "    # Calculate the maximum tokens that can be processed per minute given the RPM constraint.\n",
    "    max_tokens_possible_by_rpm = max_rpm * avg_tokens_per_profile\n",
    "    \n",
    "    if max_tokens_possible_by_rpm <= max_tokens_per_minute:\n",
    "        # RPM is the limiting factor.\n",
    "        num_batches = total_processing_ops / max_rpm\n",
    "    else:\n",
    "        # Tokens per minute is the limiting factor.\n",
    "        num_batches = est_total_tokens / max_tokens_per_minute\n",
    "    \n",
    "    # Calculate the total estimated time in minutes.\n",
    "    est_minutes = int(num_batches)\n",
    "    \n",
    "    # Convert minutes into hours and remaining minutes.\n",
    "    est_hours = est_minutes // 60\n",
    "    est_minutes = est_minutes % 60\n",
    "    \n",
    "    return est_hours, est_minutes  # Return the estimated hours and minutes required.\n",
    "\n",
    "\n",
    "# Define the model to use for generating alignment scores\n",
    "'''\n",
    "Note: ALWAYS USE THE 'gpt-4o-mini' MODEL IF YOU ARE ONLY TESTING THE CODE. \n",
    "It's 50x cheaper than the 'gpt-4o' model, and if you are running/modifying the code, \n",
    "you will likely need to run it over and over, and the output matters less. \n",
    "Switch to 'gpt-4o' if the primary focus is the output.\n",
    "'''\n",
    "model = 'gpt-4o-mini'  # Set the model to 'gpt-4o-mini' for testing or 'gpt-4o' for final runs.\n",
    "API_tier = 3  # Set the API tier based on your OpenAI account level.\n",
    "\n",
    "\n",
    "# Define the maximum number of requests per minute to avoid rate limiting\n",
    "'''\n",
    "Note: Despite being on a higher API tier, I experienced rate limiting at 200 requests per minute. \n",
    "To avoid issues, I kept it at 150 requests per minute as a safe threshold. \n",
    "This rate allows processing of 150 rows per minute, which is still efficient, but requires some wait time between batches.\n",
    "At Tier 2, you should get 5000 requests per minute, but the limit was still 200 for me. \n",
    "The issue might be related to the per-second rate of sending requests. \n",
    "Someone will need to troubleshoot this if higher throughput is required.\n",
    "'''\n",
    "max_rpm = 150  # Set the maximum requests per minute to avoid rate limiting.\n",
    "\n",
    "\n",
    "# Start the timer to measure the time taken for the entire process.\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Create the assistants with specific instructions and parameters.\n",
    "# Each assistant is tailored to extract different types of information from profiles.\n",
    "#region Assistants\n",
    "# Assistant for extracting technical skills and education from student profiles.\n",
    "S_TS_Ed_EOB = client.beta.assistants.create(\n",
    "    name=\"Student Profile Technical Skill and Education Extract-O-Bot\",  # Name of the assistant.\n",
    "    instructions=S_TS_Ed_EOB_instructions,  # Instructions specific to this task.\n",
    "    model=model,  # Model to use for this assistant.\n",
    "    temperature=1,  # Sampling temperature for generating diverse outputs.\n",
    "    top_p=1  # Nucleus sampling parameter, ensures diverse outputs.\n",
    ")\n",
    "\n",
    "# Assistant for extracting industry interests, soft skills, and values from student profiles.\n",
    "S_In_SS_V_EOB = client.beta.assistants.create(\n",
    "    name=\"Student Profile Industry of Interest, Soft Skills, and Values Extract-O-Bot\",  # Name of the assistant.\n",
    "    instructions=S_In_SS_V_EOB_instructions,  # Instructions specific to this task.\n",
    "    model=model,  # Model to use for this assistant.\n",
    "    temperature=1,  # Sampling temperature for generating diverse outputs.\n",
    "    top_p=1  # Nucleus sampling parameter, ensures diverse outputs.\n",
    ")\n",
    "\n",
    "# Assistant for extracting technical skills from job position profiles.\n",
    "P_TS_EOB = client.beta.assistants.create(\n",
    "    name=\"Position Profile Technical Skill Extract-O-Bot\",  # Name of the assistant.\n",
    "    instructions=P_TS_EOB_instructions,  # Instructions specific to this task.\n",
    "    model=model,  # Model to use for this assistant.\n",
    "    temperature=1,  # Sampling temperature for generating diverse outputs.\n",
    "    top_p=1  # Nucleus sampling parameter, ensures diverse outputs.\n",
    ")\n",
    "\n",
    "# Assistant for extracting industry, soft skills, and company values from job position profiles.\n",
    "P_In_SS_V_EOB = client.beta.assistants.create(\n",
    "    name=\"Position Profile Industry of Interest, Soft Skills, and Company Values Extract-O-Bot\",  # Name of the assistant.\n",
    "    instructions=P_In_SS_V_EOB_instructions,  # Instructions specific to this task.\n",
    "    model=model,  # Model to use for this assistant.\n",
    "    temperature=1,  # Sampling temperature for generating diverse outputs.\n",
    "    top_p=1  # Nucleus sampling parameter, ensures diverse outputs.\n",
    ")\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region token and time estimation\n",
    "\n",
    "# Calculate the total number of keyword extractions needed. \n",
    "# Each profile is processed twice for different categories, hence the multiplication by 2.\n",
    "total_keyword_extractions = (SP['pos_(Do Not Modify) Job Posting'].nunique() + SP['stu_(Do Not Modify) Application'].nunique()) * 2\n",
    "\n",
    "#region tokens per minute\n",
    "''' \n",
    "Note: The maximum number of tokens per minute varies depending on your OpenAI API tier and the model used.\n",
    "Info on tokens per minute for different models and API tiers can be found here: https://platform.openai.com/docs/guides/rate-limits\n",
    "Info on your specific API tier and rate limits can be found in the 'Your Profile' section under 'Limits' on the OpenAI platform.\n",
    "If you are using tier 1, doing a large number of comparisons will be slooowww (2+ hours for 1000 comparisons). You can only process about 4-5 rows a minute. \n",
    "Once you are tier 2 or higher, you can process 150 rows a minute and 1000 rows in 7 minutes.\n",
    "'''\n",
    "\n",
    "# Determine max tokens per minute based on the model and API tier\n",
    "if model == 'gpt-4o-mini':\n",
    "    if API_tier == 1:\n",
    "        max_tokens_per_minute = 200000  # Tier 1 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 2:\n",
    "        max_tokens_per_minute = 2000000  # Tier 2 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 3:\n",
    "        max_tokens_per_minute = 4000000  # Tier 3 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 4:\n",
    "        max_tokens_per_minute = 10000000  # Tier 4 limit for 'gpt-4o-mini'\n",
    "    elif API_tier == 5:\n",
    "        max_tokens_per_minute = 150000000  # Tier 5 limit for 'gpt-4o-mini'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported API tier for 'gpt-4o-mini'\")\n",
    "\n",
    "elif model == 'gpt-4o':\n",
    "    if API_tier == 1:\n",
    "        max_tokens_per_minute = 25000  # Tier 1 limit for 'gpt-4o'\n",
    "    elif API_tier == 2:\n",
    "        max_tokens_per_minute = 450000  # Tier 2 limit for 'gpt-4o'\n",
    "    elif API_tier == 3:\n",
    "        max_tokens_per_minute = 800000  # Tier 3 limit for 'gpt-4o'\n",
    "    elif API_tier == 4:\n",
    "        max_tokens_per_minute = 2000000  # Tier 4 limit for 'gpt-4o'\n",
    "    elif API_tier == 5:\n",
    "        max_tokens_per_minute = 30000000  # Tier 5 limit for 'gpt-4o'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported API tier for 'gpt-4o'\")\n",
    "\n",
    "else:\n",
    "    print('I only made code for gpt-4o and gpt-4o-mini. If you want to use a different model, you will have to add the token limits yourself.')\n",
    "    token_limit_input = input('Enter a token limit per minute for the model you are using: ')\n",
    "    \n",
    "    # Convert the input to an integer and set it as the max_tokens_per_minute\n",
    "    try:\n",
    "        max_tokens_per_minute = int(token_limit_input.replace(\",\", \"\").strip())  # Remove commas if present and convert to integer\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Invalid input. Please enter a valid integer for the token limit.\")\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region Actual tokens used columns\n",
    "# Initialize columns to track the actual tokens used during processing for each category.\n",
    "SP['Actual_Tokens_Used_S_TS_Ed'] = 0\n",
    "SP['Actual_Tokens_Used_S_In_SS_V'] = 0\n",
    "SP['Actual_Tokens_Used_P_TS'] = 0\n",
    "SP['Actual_Tokens_Used_P_In_SS_V'] = 0\n",
    "SP['Actual_Input_Tokens_Used_S_TS_Ed'] = 0\n",
    "SP['Actual_Input_Tokens_Used_S_In_SS_V'] = 0\n",
    "SP['Actual_Input_Tokens_Used_P_TS'] = 0\n",
    "SP['Actual_Input_Tokens_Used_P_In_SS_V'] = 0\n",
    "SP['Actual_Output_Tokens_Used_S_TS_Ed'] = 0\n",
    "SP['Actual_Output_Tokens_Used_S_In_SS_V'] = 0\n",
    "SP['Actual_Output_Tokens_Used_P_TS'] = 0\n",
    "#endregion\n",
    "\n",
    "#region token count columns\n",
    "# Initialize columns to track the estimated token count for each profile type and category.\n",
    "SP['TokenCount_S_TS_Ed'] = 0\n",
    "SP['TokenCount_S_In_SS_V'] = 0\n",
    "SP['TokenCount_P_TS'] = 0\n",
    "SP['TokenCount_P_In_SS_V'] = 0\n",
    "#endregion\n",
    "\n",
    "# Calculate the token count for student and position profiles.\n",
    "SP = calculate_profile_tokens(SP, 'stu', model)\n",
    "SP = calculate_profile_tokens(SP, 'pos', model)\n",
    "\n",
    "#region token use and price estimation\n",
    "\n",
    "# Define the relevant columns for token count and usage tracking.\n",
    "token_cols = ['TokenCount_S_TS_Ed', \n",
    "              'TokenCount_S_In_SS_V', \n",
    "              'TokenInputCount_S_TS_Ed', \n",
    "              'TokenInputCount_S_In_SS_V',\n",
    "              'TokenOutputCount_S_TS_Ed',\n",
    "              'TokenOutputCount_S_In_SS_V',\n",
    "              'TokenCount_P_TS', \n",
    "              'TokenCount_P_In_SS_V',  \n",
    "              'TokenInputCount_P_TS', \n",
    "              'TokenInputCount_P_In_SS_V',\n",
    "              'TokenOutputCount_P_TS',\n",
    "              'TokenOutputCount_P_In_SS_V']\n",
    "\n",
    "# Sum tokens for position profiles\n",
    "pos_columns = ['pos_(Do Not Modify) Job Posting'] + token_cols[6:]  # Only include token columns related to position profiles.\n",
    "pos_token_subset = SP[pos_columns].drop_duplicates()  # Drop duplicates to avoid double counting.\n",
    "\n",
    "total_tokens_P_TS = pos_token_subset['TokenCount_P_TS'].sum()\n",
    "total_tokens_P_In_SS_V = pos_token_subset['TokenCount_P_In_SS_V'].sum()\n",
    "total_tokens_P = total_tokens_P_TS + total_tokens_P_In_SS_V\n",
    "\n",
    "total_input_tokens_P_TS = pos_token_subset['TokenInputCount_P_TS'].sum()\n",
    "total_input_tokens_P_In_SS_V = pos_token_subset['TokenInputCount_P_In_SS_V'].sum()\n",
    "total_input_tokens_P = total_input_tokens_P_TS + total_input_tokens_P_In_SS_V\n",
    "\n",
    "total_output_tokens_P_TS = pos_token_subset['TokenOutputCount_P_TS'].sum()\n",
    "total_output_tokens_P_In_SS_V = pos_token_subset['TokenOutputCount_P_In_SS_V'].sum()\n",
    "total_output_tokens_P = total_output_tokens_P_TS + total_output_tokens_P_In_SS_V\n",
    "\n",
    "# Sum tokens for student profiles\n",
    "stu_columns = ['stu_(Do Not Modify) Application'] + token_cols[:6]  # Only include token columns related to student profiles.\n",
    "stu_token_subset = SP[stu_columns].drop_duplicates()  # Drop duplicates to avoid double counting.\n",
    "total_tokens_S_TS_Ed = stu_token_subset['TokenCount_S_TS_Ed'].sum()\n",
    "total_tokens_S_In_SS_V = stu_token_subset['TokenCount_S_In_SS_V'].sum()\n",
    "total_tokens_S = total_tokens_S_TS_Ed + total_tokens_S_In_SS_V\n",
    "\n",
    "total_input_tokens_S_TS_Ed = stu_token_subset['TokenInputCount_S_TS_Ed'].sum()\n",
    "total_input_tokens_S_In_SS_V = stu_token_subset['TokenInputCount_S_In_SS_V'].sum()\n",
    "total_input_tokens_S = total_input_tokens_S_TS_Ed + total_input_tokens_S_In_SS_V\n",
    "\n",
    "total_output_tokens_S_TS_Ed = stu_token_subset['TokenOutputCount_S_TS_Ed'].sum()\n",
    "total_output_tokens_S_In_SS_V = stu_token_subset['TokenOutputCount_S_In_SS_V'].sum()\n",
    "total_output_tokens_S = total_output_tokens_S_TS_Ed + total_output_tokens_S_In_SS_V\n",
    "    \n",
    "# Compute the total token count for all profiles (position + student).\n",
    "est_total_tokens = total_tokens_P + total_tokens_S\n",
    "total_input_tokens = total_input_tokens_P + total_input_tokens_S\n",
    "total_output_tokens = total_output_tokens_P + total_output_tokens_S\n",
    "\n",
    "# Compute the max token count for all profiles to identify the most demanding profile in terms of tokens.\n",
    "max_token_value = SP[token_cols].max().max()\n",
    "\n",
    "#region cost per token\n",
    "# Set the cost per token based on the model used.\n",
    "if model == 'gpt-4o-mini':\n",
    "    price_per_input_token = 0.00000015\n",
    "    price_per_output_token = 0.00000030\n",
    "elif model == 'gpt-4o':\n",
    "    price_per_input_token = 0.000005\n",
    "    price_per_output_token = 0.000015\n",
    "\n",
    "# Estimate the total cost for processing based on token usage.\n",
    "est_total_price = (total_input_tokens * price_per_input_token) + (total_output_tokens * price_per_output_token)\n",
    "#endregion\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region time estimation\n",
    "# Estimate the processing time based on the number of tokens, tokens per minute, and requests per minute limits.\n",
    "est_hours, est_minutes = estimate_processing_time(SP, est_total_tokens, max_tokens_per_minute, max_rpm)\n",
    "#endregion\n",
    "\n",
    "# Ensure a minimum processing time of 4 minutes, even if the estimated time is less.\n",
    "if est_hours == 0 and est_minutes < 3:\n",
    "    est_minutes = 4\n",
    "\n",
    "#region Display the token and time estimation results\n",
    "# Display the estimated number of profiles, tokens, price, and processing time.\n",
    "total_rows_label = widgets.HTML(value=f\"<b>Total number of profiles to process:</b> {total_keyword_extractions}\")\n",
    "est_tokens_label = widgets.HTML(value=f\"<b>Estimate for total number of tokens required:</b> {est_total_tokens}\")\n",
    "est_price_label = widgets.HTML(value=f\"<b>Estimate for total price for processing:</b> ${est_total_price:.2f}\")\n",
    "est_time_label = widgets.HTML(value=f\"<b>Estimated time required:</b> {est_hours} hours and {est_minutes} minutes\")\n",
    "display(total_rows_label, est_tokens_label, est_price_label, est_time_label)\n",
    "#endregion\n",
    "\n",
    "\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region keywords columns\n",
    "# Initialize columns to store extracted keywords for both student and position profiles.\n",
    "SP.loc[:, 'pos_technical_keywords'] = None\n",
    "SP.loc[:, 'stu_technical_keywords'] = None\n",
    "SP.loc[:, 'pos_industry_keywords'] = None\n",
    "SP.loc[:, 'stu_industry_keywords'] = None\n",
    "SP.loc[:, 'pos_soft_keywords'] = None\n",
    "SP.loc[:, 'stu_soft_keywords'] = None\n",
    "SP.loc[:, 'pos_values_keywords'] = None\n",
    "SP.loc[:, 'stu_values_keywords'] = None\n",
    "SP.loc[:, 'stu_education_keywords'] = None\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region processed flag columns\n",
    "# Initialize columns to track whether each profile has been processed for each category.\n",
    "SP['S_TS_Ed_processed'] = False\n",
    "SP['S_In_SS_V_processed'] = False\n",
    "SP['P_TS_processed'] = False\n",
    "SP['P_In_SS_V_processed'] = False\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region Global variables\n",
    "# Initialize global variables to track the total tokens used and the number of keyword extractions performed.\n",
    "total_tokens_used = 0\n",
    "total_input_tokens_used = 0\n",
    "total_output_tokens_used = 0\n",
    "num_keyword_extractions = 0\n",
    "threads_in_loop = 0\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region Threads for resource management\n",
    "# Define resource limits and thread management variables.\n",
    "rps = 80  # Requests per second limit.\n",
    "tps = 66666  # Tokens per second limit.\n",
    "threads_lock = threading.Lock()  # Lock for managing access to shared resources.\n",
    "rate_limit = threading.Semaphore(rps)  # Semaphore for managing request rate limits.\n",
    "token_limit = threading.Semaphore(tps)  # Semaphore for managing token usage limits.\n",
    "threading.Thread(target=replenish_rps, daemon=True).start()  # Start thread to replenish request rate limits.\n",
    "threading.Thread(target=replenish_tps, daemon=True).start()  # Start thread to replenish token limits.\n",
    "batch_completed = threading.Event()  # Event to signal batch completion.\n",
    "requests_started = threading.Event()  # Event to signal that requests have started.\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region thread monitoring\n",
    "# # For monitoring the number of threads waiting for API calls to be completed. Commented out for now, but useful for debugging.\n",
    "# # Variable to track the number of threads currently processing.\n",
    "# def monitor_loop_threads():\n",
    "#     while True:\n",
    "#         with threads_lock:\n",
    "#             print(f\"Threads in loop: {threads_in_loop}\")\n",
    "#         time.sleep(5)  # Print the number of threads in the loop every 5 seconds\n",
    "\n",
    "# # Start the loop monitoring in a background thread\n",
    "# threading.Thread(target=monitor_loop_threads, daemon=True).start()\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region define paths for saving\n",
    "# Define directories for saving intermediate and final results.\n",
    "timeout_dir = f'{path_to_project}/data/extract_o_bot_saves/timeout_dir'\n",
    "periodic_save_dir = f'{path_to_project}/data/extract_o_bot_saves/periodic_save_dir'\n",
    "final_save_path = f'{path_to_project}/data/extract_o_bot_saves/final_save_dir/SP_extraction_save.parquet'\n",
    "#endregion\n",
    "\n",
    "\n",
    "# Run the keyword extraction process, displaying a progress bar.\n",
    "with tqdm(total=total_keyword_extractions, desc=\"Extracting keywords\") as pbar:\n",
    "    execute_keyword_extractions(SP, pbar, timeout_dir, periodic_save_dir, 'SP_extraction_temp', max_tokens_per_minute, max_rpm)\n",
    "\n",
    "\n",
    "#region Final token and time calculation\n",
    "total_price = (total_input_tokens_used * price_per_input_token) + (total_output_tokens_used * price_per_output_token)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_hours = int(elapsed_time // 3600)\n",
    "elapsed_minutes = int((elapsed_time % 3600) // 60)\n",
    "elapsed_seconds = int(elapsed_time % 60)\n",
    "formatted_time = f\"{elapsed_hours}h {elapsed_minutes}m {elapsed_seconds}s\"\n",
    "process_complete_label = widgets.HTML(value=f\"<b>Processing complete. Keywords extracted from {total_keyword_extractions} profiles. Final DataFrame saved to:</b> {final_save_path}\")\n",
    "tokens_label = widgets.HTML(value=f\"<b>Total tokens used:</b> {total_tokens_used}\")\n",
    "price_label = widgets.HTML(value=f\"<b>Total price for processing:</b> ${total_price:.2f}\")\n",
    "time_label = widgets.HTML(value=f\"<b>Total time used:</b> {formatted_time}\")\n",
    "#endregion\n",
    "\n",
    "\n",
    "#region final save\n",
    "\n",
    "# combine the finished output with any previously saved output\n",
    "combined_df = load_and_combine_saved_dfs(path, 'SP_extraction_save')\n",
    "\n",
    "# if combinded_df is empty, save the final DataFrame\n",
    "if combined_df.empty:\n",
    "    SP.to_parquet(final_save_path)   \n",
    "else: \n",
    "    final_df = pd.concat([SP, combined_df], ignore_index=True).drop_duplicates()\n",
    "    final_df.to_parquet(final_save_path)\n",
    "    \n",
    "#endregion\n",
    "\n",
    "\n",
    "display(process_complete_label, tokens_label, price_label, time_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract-O-Bot Output Analysis</h3>\n",
    "\n",
    "<p>In this section, we will delve into the results generated by the Extract-O-Bot.</p>\n",
    "\n",
    "<p>The table we're working with contains keyword extractions generated using the GPT-4o-mini model, encompassing all student and company profiles from 2024. This table will also serve as the foundation for Pipeline 3, where we will evaluate and score the extracted keywords.</p>\n",
    "\n",
    "<p><strong>Note:</strong> I opted to use GPT-4o-mini for the AI-generated tables to avoid the cost of using GPT-4o. However, I’ve found that GPT-4o provides significantly better extraction results. The tables in the Technical White Paper were generated using GPT-4o. If you'd like to generate and view results using GPT-4o, run a sample with Extract-O-Bot. Be sure to comment out SP_path and uncomment SP_path_final_save for the final output.</p>\n",
    "\n",
    "<p>Exploring these results is critical for understanding the accuracy and relevance of the extracted keywords, evaluating how well the extracted data aligns with the intended categorization criteria, and identify any potential areas for improvement. By analyzing the output, we aim to ensure that the keyword extraction process not only meets the predefined guidelines but also adds significant value to the matching process.This analysis is crucial for validating the Extract-O-Bot's performance and ensuring that the keyword extraction process effectively supports the broader objective of matching students with suitable job opportunities. By continuously evaluating and refining the output, we can enhance the overall efficiency and reliability of the pipeline.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_path = f'{path_to_project}/data/SP_table/SP2_post_extract_o_bot.parquet'\n",
    "SP = pd.read_parquet(SP_path) \n",
    "\n",
    "# SP_path_final_save = f'{path_to_project}/data/extract_o_bot_saves/final_save_dir/SP_extraction_save.parquet'\n",
    "# SP = pd.read_parquet(SP_path_final_save) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_cols = '''\n",
    "stu_Legal Name\n",
    "stu_technical_keywords\n",
    "stu_industry_keywords\n",
    "stu_soft_keywords\n",
    "stu_values_keywords\n",
    "stu_education_keywords\n",
    "'''\n",
    "\n",
    "pos_cols = '''\n",
    "pos_Company\n",
    "pos_Name\n",
    "pos_technical_keywords\n",
    "pos_industry_keywords\n",
    "pos_soft_keywords\n",
    "pos_values_keywords\n",
    "'''\n",
    "\n",
    "stu_cols = as_list(stu_cols)\n",
    "pos_cols = as_list(pos_cols)\n",
    "\n",
    "stu_keywords = SP[stu_cols]\n",
    "pos_keywords = SP[pos_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Examples of Keyword Extraction and Labels from Students and Companies</h3>\n",
    "\n",
    "<p>The tables below present examples of some of the keywords extracted by the Extract-O-Bot. These examples include both student and company profiles, showcasing the extracted keywords along with their assigned labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Student Keywords:')\n",
    "stu_keywords.sample(10)\n",
    "pretty_print(stu_keywords.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Position Profile Keywords:\")\n",
    "pos_keywords.sample(10)\n",
    "pretty_print(pos_keywords.sample(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
